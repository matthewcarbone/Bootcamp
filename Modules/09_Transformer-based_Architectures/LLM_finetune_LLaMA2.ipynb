{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d895c021-2d9f-4eca-8686-da14ffe6e996",
   "metadata": {},
   "source": [
    "# Finetunning LLaMA 2 7B step by step\n",
    "In this notebooks you will learn how to finetune a pretrained LLama model on an Instruction dataset. \n",
    "> This notebook requires a A100 or V100 GPU with at least 24GB of memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed61ee90-cda8-43f4-80d4-71547edbf117",
   "metadata": {},
   "source": [
    "## Prepare your Instruction Dataset\n",
    "An Instruction dataset is a list of instructions/outputs pairs that are relevant to your own domain.<br> \n",
    "For instance it could be question and answers from an specific domain, problems and solution for a technical domain, or just instruction and outputs.<br>\n",
    "Let's grab the Alpaca (GPT-4 curated instructions and outputs) dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bc50db7-2c56-40e6-bce9-cb9c4f6a75f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset dataset json file\n",
    "import json\n",
    "\n",
    "dataset_file = \"alpaca_gpt4_data.json\"\n",
    "\n",
    "with open(dataset_file, \"r\") as f:\n",
    "    alpaca = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d44b6c26-091a-4e09-b6c2-c79b62d40bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and validation set split\n",
    "import random\n",
    "\n",
    "seed = 42\n",
    "#shuffle the datas first\n",
    "random.seed(seed)\n",
    "random.shuffle(alpaca)  \n",
    "\n",
    "train_dataset = alpaca[:-1000]\n",
    "eval_dataset = alpaca[-1000:] #1000 samples for validation\n",
    "\n",
    "import pandas as pd\n",
    "# saving the traina and eval jsonal file for next training and reproduce the resutls\n",
    "train_df = pd.DataFrame(train_dataset)\n",
    "eval_df = pd.DataFrame(eval_dataset)\n",
    "train_df.to_json(\"alpaca_gpt4_train.jsonl\", orient='records', lines=True)\n",
    "eval_df.to_json(\"alpaca_gpt4_eval.jsonl\", orient='records', lines=True)\n",
    "\n",
    "def load_jsonl(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            data.append(json.loads(line))\n",
    "    return data\n",
    "    \n",
    "train_dataset = load_jsonl(f\"alpaca_gpt4_train.jsonl\")\n",
    "eval_dataset = load_jsonl(f\"alpaca_gpt4_eval.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0723b60-7558-4a55-92f8-cb85e8d57b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing the data by adding the prompt (some LLM has default prompt, you need add them in front of instruction.\n",
    "#For alpaca dataset, some samples have input while others don't, thus we generate two different prompt.\n",
    "def prompt_no_input(row):\n",
    "    return (\"Below is an instruction that describes a task. \"\n",
    "            \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "            \"### Instruction:\\n{instruction}\\n\\n### Response:\\n\").format_map(row)\n",
    "\n",
    "def prompt_input(row):\n",
    "    return (\"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "            \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "            \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\\n\").format_map(row)\n",
    "\n",
    "def create_alpaca_prompt(row):\n",
    "    return prompt_no_input(row) if row[\"input\"] == \"\" else prompt_input(row)\n",
    "\n",
    "train_prompts = [create_alpaca_prompt(row) for row in train_dataset]\n",
    "eval_prompts = [create_alpaca_prompt(row) for row in eval_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68eb5d2a-39a9-47f6-b7ad-3468ed60bff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing the data by adding EOS token.\n",
    "def pad_eos(ds):\n",
    "    EOS_TOKEN = \"</s>\"\n",
    "    return [f\"{row['output']}{EOS_TOKEN}\" for row in ds]\n",
    "train_outputs = pad_eos(train_dataset)\n",
    "eval_outputs = pad_eos(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "101757ce-28af-4b71-855d-928d84dfc545",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finish the data preprocesing by adding prompt in front of data and EOS in the end of data\n",
    "train_dataset = [{\"prompt\":s, \"output\":t, \"example\": s + t} for s, t in zip(train_prompts, train_outputs)]\n",
    "eval_dataset = [{\"prompt\":s, \"output\":t, \"example\": s + t} for s, t in zip(eval_prompts, eval_outputs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b85cc6d-835d-4a94-aef9-c9b58cb003c6",
   "metadata": {},
   "source": [
    "## Converting text to numbers: Tokenizer\n",
    "We need to convert the dataset into tokens, you can quickly do this with the workhorse of the transformers library, the Tokenizer! This function does a lot of heavy lifting besides tokenizing the text. \n",
    "\n",
    "- It tokenizes the text\n",
    "- Converts the outputs to PyTorch tensors\n",
    "- Pads the inputs to match length and more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f875a4c-9478-4fe3-832a-3dbcfb78c6fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/idies/workspace/Storage/xyu1/persistent/pytorch_env/llava/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens: 11499843\n",
      "Total number of tokens: 216533\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11219"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "model_id = '/home/idies/workspace/Temporary/xyu1/scratch/hub/Llama-2-7b-hf'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "max_sequence_len = 1024\n",
    "\n",
    "def pack(dataset, max_seq_len=max_sequence_len):\n",
    "    #It tokenizes the text\n",
    "    tkds_ids = tokenizer([s[\"example\"] for s in dataset])[\"input_ids\"]\n",
    "    \n",
    "    all_token_ids = []\n",
    "    for tokenized_input in tkds_ids:\n",
    "        all_token_ids.extend(tokenized_input)# + [tokenizer.eos_token_id])\n",
    "    \n",
    "    print(f\"Total number of tokens: {len(all_token_ids)}\")\n",
    "    #Pads the inputs to match length and more!\n",
    "    packed_ds = []\n",
    "    for i in range(0, len(all_token_ids), max_seq_len+1):\n",
    "        input_ids = all_token_ids[i : i + max_seq_len+1]\n",
    "        if len(input_ids) == (max_seq_len+1):\n",
    "            packed_ds.append({\"input_ids\": input_ids[:-1], \"labels\": input_ids[1:]})  # this shift is not needed if using the model.loss\n",
    "    return packed_ds\n",
    "\n",
    "\n",
    "train_ds_packed = pack(train_dataset)\n",
    "eval_ds_packed = pack(eval_dataset)\n",
    "len(train_ds_packed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479a1e4c-8a22-4e2a-a46d-4286c841a31d",
   "metadata": {},
   "source": [
    "## Before training\n",
    "We need to prepare the dataloader, inputs and labels, optimiztion, loss function\n",
    "\n",
    "- Dataloader\n",
    "- hyperparameters\n",
    "- Load pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c50503c1-ba0a-4150-83bd-7cb727b94684",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import default_data_collator\n",
    "\n",
    "#datalaoder for training and testing\n",
    "torch.manual_seed(seed)\n",
    "batch_size = 16  # I have an A100 GPU with 40GB of RAM \n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_ds_packed,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=default_data_collator, # we don't need any special collator \n",
    ")\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "    eval_ds_packed,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=default_data_collator,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "34b52815-9fc1-4111-a6c1-6432b91c6e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "#storing all my hyperparameters into a SimpleNamespace,\n",
    "gradient_accumulation_steps = 2\n",
    "\n",
    "config = SimpleNamespace(\n",
    "    model_id=model_id,\n",
    "    dataset_name=\"alpaca-gpt4\",\n",
    "    precision=\"bf16\",  # faster and better than fp16, requires new GPUs\n",
    "    n_freeze=24,  # How many layers we don't train, LLama 7B has 32.\n",
    "    lr=2e-4,\n",
    "    n_eval_samples=10, # How many samples to generate on validation\n",
    "    max_seq_len=max_sequence_len, # Lenght of the sequences to pack\n",
    "    epochs=3,  # we do 3 pasess over the dataset.\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,  # evey how many iterations we update the gradients, simulates larger batch sizes\n",
    "    batch_size=batch_size,  # what my GPU can handle, depends on how many layers are we training  \n",
    "    log_model=False,  # upload the model to W&B?\n",
    "    gradient_checkpointing = True,  # saves even more memory\n",
    "    freeze_embed = True,  # why train this? let's keep them frozen ❄️\n",
    "    seed=seed,\n",
    ")\n",
    "\n",
    "config.total_train_steps = config.epochs * len(train_dataloader) // config.gradient_accumulation_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba69ccd7-0ba0-40f7-9db3-1dd8adad5101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-08-01 22:06:30,519] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.30s/it]\n"
     ]
    }
   ],
   "source": [
    "#Load pretrained model of LLaMA2 7B from HuggingFace\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.model_id,\n",
    "    device_map=0,\n",
    "    trust_remote_code=True,\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    use_cache=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2142ca92-f51d-46f7-ad51-23ef51efff88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To make training fast and using small GPU memory, we freeze the layers\n",
    "# freeze layers (disable gradients)\n",
    "for param in model.parameters(): param.requires_grad = False\n",
    "for param in model.lm_head.parameters(): param.requires_grad = True\n",
    "for param in model.model.layers[config.n_freeze:].parameters(): param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bbdced62-6fa7-42d9-af78-78e5ac91aed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 6738.42M, Trainable: 1750.14M\n"
     ]
    }
   ],
   "source": [
    "#count the the model trainable parameters\n",
    "def param_count(m):\n",
    "    params = sum([p.numel() for p in m.parameters()])/1_000_000\n",
    "    trainable_params = sum([p.numel() for p in m.parameters() if p.requires_grad])/1_000_000\n",
    "    print(f\"Total params: {params:.2f}M, Trainable: {trainable_params:.2f}M\")\n",
    "    return params, trainable_params\n",
    "\n",
    "params, trainable_params = param_count(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "abb3fa82-0ec2-438e-9818-7727ca5ef2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just freeze embeddings for small memory decrease\n",
    "if config.freeze_embed:\n",
    "    model.model.embed_tokens.weight.requires_grad_(False);\n",
    "# save more memory and you can also use gradient checkpointing to save even more (this makes training slower, how much it will depend on your particular configuration). \n",
    "#There is a [nice article](https://huggingface.co/docs/transformers/v4.18.0/en/performance) on the Huggingface website about how to fit large models on memory, \n",
    "#I encourage you to check it!\n",
    "if config.gradient_checkpointing:\n",
    "    model.gradient_checkpointing_enable()\n",
    "\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "#optimizer\n",
    "optim = torch.optim.Adam(model.parameters(), lr=config.lr, betas=(0.9,0.99), eps=1e-5)\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optim,\n",
    "    num_training_steps=config.total_train_steps,\n",
    "    num_warmup_steps=config.total_train_steps // 10,\n",
    ")\n",
    "#loss function\n",
    "def loss_fn(x, y):\n",
    "    \"A Flat CrossEntropy\" \n",
    "    return torch.nn.functional.cross_entropy(x.view(-1, x.shape[-1]), y.view(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73c4deb-7ff8-4de7-a37d-cd43cdcb8c38",
   "metadata": {},
   "source": [
    "## Testing during training\n",
    "\n",
    "We are almost there, let's create a simple function to sample from the model now and then, to visualy see what the models is outputting!<br>\n",
    "Let's wrap the model.generate method for simplicity. You can grab the defaults sampling parameters from the GenerationConfig and passing the corresponding model_id. This will grab you the defaults for parameters like temperature, top p, etc...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "54f64256-cf85-494a-9f34-9cb11ec1a923",
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "from transformers import GenerationConfig\n",
    "import wandb\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "gen_config = GenerationConfig.from_pretrained(config.model_id)\n",
    "test_config = SimpleNamespace(\n",
    "    max_new_tokens=256,\n",
    "    gen_config=gen_config)\n",
    "\n",
    "def generate(prompt, max_new_tokens=test_config.max_new_tokens, gen_config=gen_config):\n",
    "    tokenized_prompt = tokenizer(prompt, return_tensors='pt')['input_ids'].cuda()\n",
    "    with torch.inference_mode():\n",
    "        output = model.generate(tokenized_prompt, \n",
    "                            max_new_tokens=max_new_tokens, \n",
    "                            generation_config=gen_config)\n",
    "    return tokenizer.decode(output[0][len(tokenized_prompt[0]):], skip_special_tokens=True)\n",
    "\n",
    "def to_gpu(tensor_dict):\n",
    "    return {k: v.to('cuda') for k, v in tensor_dict.items()}\n",
    "\n",
    "class Accuracy:\n",
    "    \"A simple Accuracy function compatible with HF models\"\n",
    "    def __init__(self):\n",
    "        self.count = 0\n",
    "        self.tp = 0.\n",
    "    def update(self, logits, labels):\n",
    "        logits, labels = logits.argmax(dim=-1).view(-1).cpu(), labels.view(-1).cpu()\n",
    "        tp = (logits == labels).sum()\n",
    "        self.count += len(logits)\n",
    "        self.tp += tp\n",
    "        return tp / len(logits)\n",
    "    def compute(self):\n",
    "        return self.tp / self.count\n",
    "\n",
    "from pathlib import Path\n",
    "def save_model(model, model_name, models_folder=\"models\", log=False):\n",
    "    \"\"\"Save the model to wandb as an artifact\n",
    "    Args:\n",
    "        model (nn.Module): Model to save.\n",
    "        model_name (str): Name of the model.\n",
    "        models_folder (str, optional): Folder to save the model. Defaults to \"models\".\n",
    "    \"\"\"\n",
    "    model_name = f\"{wandb.run.id}_{model_name}\"\n",
    "    file_name = Path(f\"{models_folder}/{model_name}\")\n",
    "    file_name.parent.mkdir(parents=True, exist_ok=True)\n",
    "    model.save_pretrained(file_name, safe_serialization=True)\n",
    "    # save tokenizer for easy inference\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model.name_or_path)\n",
    "    tokenizer.save_pretrained(model_name)\n",
    "    if log:\n",
    "        at = wandb.Artifact(model_name, type=\"model\")\n",
    "        at.add_dir(file_name)\n",
    "        wandb.log_artifact(at)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f0ae08e5-bc6f-4fee-8055-c29746ae327a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def validate():\n",
    "    model.eval();\n",
    "    eval_acc = Accuracy()\n",
    "    loss, total_steps = 0., 0\n",
    "    for step, batch in enumerate(pbar:=tqdm(eval_dataloader, leave=False)):\n",
    "        pbar.set_description(f\"doing validation\")\n",
    "        batch = to_gpu(batch)\n",
    "        total_steps += 1\n",
    "        with torch.amp.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "            out = model(**batch)\n",
    "            loss += loss_fn(out.logits, batch[\"labels\"])  # you could use out.loss and not shift the dataset\n",
    "        eval_acc.update(out.logits, batch[\"labels\"])\n",
    "    # we log results at the end\n",
    "    wandb.log({\"eval/loss\": loss.item() / total_steps,\n",
    "               \"eval/accuracy\": eval_acc.compute()})\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4899ec20-ccf4-4061-86a4-1daea18c4dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The actual Loop\n",
    "It's actually nothing fancy, and very short! It has:\n",
    "- Gradient accumulation and gradient scaling\n",
    "- sampling and model checkpoint saving (this trains very fast, no need to save multiple checkpoints)\n",
    "- We compute token accuracy, better metric than loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520d5432-e75c-44e3-a857-cf2d1badd1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"LLM_tutorial_alpaca_ft\", # the project I am working on\n",
    "           tags=[\"baseline\",\"7b\"],\n",
    "           job_type=\"train\",\n",
    "           config=config) # the Hyperparameters I want to keep track of\n",
    "\n",
    "# Training\n",
    "acc = Accuracy()\n",
    "model.train()\n",
    "train_step = 0\n",
    "for epoch in tqdm(range(config.epochs)):\n",
    "    for step, batch in enumerate(tqdm(train_dataloader)):\n",
    "        batch = to_gpu(batch)\n",
    "        with torch.amp.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "            out = model(**batch)\n",
    "            loss = loss_fn(out.logits, batch[\"labels\"]) / config.gradient_accumulation_steps  # you could use out.loss and not shift the dataset  \n",
    "            loss.backward()\n",
    "        if step%config.gradient_accumulation_steps == 0:\n",
    "            # we can log the metrics to W&B\n",
    "            wandb.log({\"train/loss\": loss.item() * config.gradient_accumulation_steps,\n",
    "                       \"train/accuracy\": acc.update(out.logits, batch[\"labels\"]),\n",
    "                       \"train/learning_rate\": scheduler.get_last_lr()[0],\n",
    "                       \"train/global_step\": train_step})\n",
    "            optim.step()\n",
    "            scheduler.step()\n",
    "            optim.zero_grad(set_to_none=True)\n",
    "            train_step += 1\n",
    "    validate()    \n",
    "\n",
    "save_model(model, model_name=config.model_id.replace(\"/\", \"_\"), models_folder=\"models/\", log=config.log_model) \n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1120d58a-a21b-4d87-9691-9ef0ac40839d",
   "metadata": {},
   "source": [
    "## Testing your new fine-tuning data\n",
    "Now you can play with your new LLM model\n",
    "- Load weights\n",
    "- Generate input and get the answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "af7f847f-1e5b-485c-a160-6848ae6dddae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    '/home/idies/workspace/Storage/xyu1/persistent/LLM_tutorial/models/0mbdgpy0__home_idies_workspace_Temporary_xyu1_scratch_hub_Llama-2-7b-hf',\n",
    "    device_map=0,\n",
    "    trust_remote_code=True,\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    use_cache=False,\n",
    ")\n",
    "\n",
    "\n",
    "def generate_test_instruction(Instruction, Input):\n",
    "    if Input is not None:\n",
    "        return {'instruction': Instruction,\n",
    "                'input': Input,\n",
    "                'output': ''}\n",
    "    else:\n",
    "        return {'instruction': Instruction,\n",
    "       'input': '',\n",
    "       'output': ''}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c87c33-625a-42e2-a6de-69864d328840",
   "metadata": {},
   "outputs": [],
   "source": [
    "Instruction = 'Suggest a good vacation plan.' #test for yourself, ask any question you want\n",
    "Input = \"Salt Lake City, Utah.\n",
    "\n",
    "prompt_customer_question = create_alpaca_prompt(generate_own_instruction(Instruction, Input))\n",
    "print(prompt_customer_question + generate(prompt_customer_question, model, 128))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
