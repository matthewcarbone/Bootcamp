<div align=center>

# Transformer-based Architectures

</div>
<span id='all_content'/>
## Content


* <a href='#environment'>1. Preparing</a>
    * <a href='#install_environment'>1.1 Environment Installation</a>
    * <a href='#download_LLaMA_model'>1.2 Prepare LLaMA2-7B Checkpoint</a>
* <a href='#train_LLM'>2. Train LLM</a>
    * <a href='#data_preparation'>2.1 Data Preparation</a>
    * <a href='#model_training'>2.2 Training LLM</a>

* <a href='#other_resources'>3. Other resources</a>


****

<span id='environment'/>

### 1. Preparing <a href='#all_catelogue'>[Back to Top]</a>

<span id='install_environment'/>

#### 1.1 Environment Installation

Clone the repository locally:

```
git clone https://github.com/CASIA-IVA-Lab/AnomalyGPT.git 
```

Install the required packages:

```
pip install -r requirements.txt
```

<span id='download_LLaMA2 7B model'/>

### 2. Train LLM <a href='#all_catelogue'>[Back to Top]</a>

## 3 Other resources <a href='#all_content'>[Back to Top]</a>
It's hard to beat existing online resources in the field of large language models. Driven by incredible industry incentive, the tech sector (Google, OpenAI, Facebook/Meta, etc.) have made enormous strides in the fundamental technology of these models. Luckily, some of its core developers have created some of the best educational content available. We curate that for you here.

### Andrej Karpathy's YouTube videos on the Transformer and GPT-2
Andrej is a legendary YouTuber and content creator, not to mention a co-founder of OpenAI and was formally a director of AI at Tesla. He has created a wealth of incredible content on his YouTube channel. Two lessons specifically are particularly instructive:
1. [Let's build GPT: from scratch, in code, spelled out](https://youtu.be/kCc8FmEb1nY?si=K-Q4p1fmdR0nbHNU) - Andrej builds and implements multiple transformer-based models on a text database containing all of [Shakespeare's works](https://github.com/karpathy/ng-video-lecture/blob/master/input.txt). Check out the "Nano GPT" lecture (basically the source code for these videos) [here](https://github.com/karpathy/ng-video-lecture).
2. [Let's reproduce GPT-2 (124M)](https://youtu.be/l8pRSuU81PU?si=gAOYf1xujRcQ_nEC) - Exactly as advertised.

