<div align=center>

# Transformer-based Architectures

</div>

# Content
TK

# Other resources
It's hard to beat existing online resources in the field of large language models. Driven by incredible industry incentive, the tech sector (Google, OpenAI, Facebook/Meta, etc.) have made enormous strides in the fundamental technology of these models. Luckily, some of its core developers have created some of the best educational content available. We curate that for you here.

## Andrej Karpathy's YouTube videos on the Transformer and GPT-2
Andrej is a legendary YouTuber and content creator, not to mention a co-founder of OpenAI and was formally a director of AI at Tesla. He has created a wealth of incredible content on his YouTube channel. Two lessons specifically are particularly instructive:
1. [Let's build GPT: from scratch, in code, spelled out](https://youtu.be/kCc8FmEb1nY?si=K-Q4p1fmdR0nbHNU) - Andrej builds and implements multiple transformer-based models on a text database containing all of [Shakespeare's works](https://github.com/karpathy/ng-video-lecture/blob/master/input.txt). Check out the "Nano GPT" lecture (basically the source code for these videos) [here](https://github.com/karpathy/ng-video-lecture).
2. [Let's reproduce GPT-2 (124M)](https://youtu.be/l8pRSuU81PU?si=gAOYf1xujRcQ_nEC) - Exactly as advertised.
