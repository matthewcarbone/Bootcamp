{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54cbaea0",
   "metadata": {},
   "source": [
    "# Regression with K-nearest neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a34df2",
   "metadata": {},
   "source": [
    "In this tutorial, we will go over K-nearest neighbors, or KNN regression, a simple machine learning algorithm that can nonetheless be used with great success. Essentially, given some unlabelled input, the KNN algorithm looks for the nearest neighbors of an input, and uses those neighbors to predict the label of the input. By the end of this tutorial, you will know:\n",
    "1. How exactly KNN algorithm works.\n",
    "2. How to process data to feed into the KNN algorithm\n",
    "3. How to tune the KNN algorithm for best performance. \n",
    "4. The benefits and limitations of the KNN algorithm\n",
    "\n",
    "In this tutorial, we'll use the KNN algorithm to predict median house prices of districts in California, as well as apply the algorithm to a condensed matter physics problem. First, if running a Google Colab instance, you'll need to pull the data manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e36d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -o spectral_data.npz https://raw.githubusercontent.com/JackieLee23/KNN-Tutorial/main/spectral_data.npz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01053b0f",
   "metadata": {},
   "source": [
    "## The Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe3364e",
   "metadata": {},
   "source": [
    "To get a sense of what the KNN algorithm is doing, we'll be working with a real life example. Let's import libraries we'll need first, and set some defaults for plotting that we'll need later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4944597",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "plt.rcParams['figure.dpi'] = 250\n",
    "plt.rcParams[\"figure.figsize\"] = [3, 2]\n",
    "plt.rcParams[\"font.size\"] = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0dd4e9",
   "metadata": {},
   "source": [
    "In this tutorial, we'll be looking at a dataset of house prices in different California districts. Given different features of houses in a district, we want to try to predict the median house price in that district. \n",
    "\n",
    "Let's start by downloading the dataset, and seeing what exactly the features are. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e648c694",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_data = fetch_california_housing(as_frame = True)\n",
    "housing_df = housing_data['frame']\n",
    "housing_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed7fe21",
   "metadata": {},
   "source": [
    "We can see that there are 9 columns total: the first 8 are the **features** of the houses in each district, while the last column is the **target** we're trying to predict - the median house price, in hundreds of thousands of dollars. I've used the data analysis library pandas to see this table - more information on pandas can be found in their [documentation](https://pandas.pydata.org/docs/user_guide/index.html). The housing_df variable is a pandas **dataframe** - essentially a table with each row being a datapoint, and the columns being different features. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a12b5a",
   "metadata": {},
   "source": [
    "## Intuition and Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67860895",
   "metadata": {},
   "source": [
    "To start, let's visualize what KNN is doing on a small subset of the data, and only look at two of the features - median income, and number of rooms. I'll use the plotting library [matplotlib](https://matplotlib.org/stable/tutorials/introductory/quick_start.html#sphx-glr-tutorials-introductory-quick-start-py) to make a scatterplot of this subset, with the points labelled and colored by the median house price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9067e7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = [0, 1, 2, 3, 4, 5, 7, 8, 10]\n",
    "x_coords = housing_df.loc[indices, 'MedInc'].to_numpy()\n",
    "y_coords = housing_df.loc[indices, 'AveRooms'].to_numpy()\n",
    "house_values = housing_df.loc[indices, 'MedHouseVal'].to_numpy()\n",
    "\n",
    "def scatter_known(ax):\n",
    "    ax.scatter(x_coords, y_coords, s = 30, c = house_values, cmap = 'magma')\n",
    "    ax.set_xlim(1.5, 9)\n",
    "    ax.set_ylim(4.0, 9.0)\n",
    "    ax.set_xlabel(\"Median Income (in $10,000)\")\n",
    "    ax.set_ylabel(\"Average Number of Rooms\")\n",
    "    ax.set_title(\"Median House Price (in $100,000)\")\n",
    "\n",
    "    for x, y, label in zip(x_coords, y_coords, house_values):\n",
    "        ax.annotate(f\"{label}\", (x, y), xytext = (x - 0.2, y + 0.22))\n",
    "        \n",
    "fig, ax = plt.subplots()\n",
    "scatter_known(ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35510cd9",
   "metadata": {},
   "source": [
    "Now here is the canonical supervised machine learning probem: given a **new** district, for which we don't know the house value, can we use the labelled data points to predict its value? I've plotted a mock new data point below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b11879a",
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_x, unknown_y = 4.5, 6\n",
    "\n",
    "def scatter_unknown(ax, label_pred = None):\n",
    "    ax.scatter([unknown_x], [unknown_y], color = \"red\", s = 30) \n",
    "    ax.annotate(\"???\", (unknown_x, unknown_y), xytext = (unknown_x - 0.1, unknown_y + 0.22), color = \"red\");\n",
    "    \n",
    "fig, ax = plt.subplots()\n",
    "scatter_known(ax)\n",
    "scatter_unknown(ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31685e7c",
   "metadata": {},
   "source": [
    "There are a lot of different ways that we could go about predicting the label for this new datapoint. One of the simplest ways is by simply guessing that this point should have roughly the same label as the closest point on the graph - the purple point with value 3.422 to the left of the red point. More formally, we're finding the closest labelled point in **feature space** (the axis being the features). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0467b3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "nearest_x, nearest_y, nearest_label = x_coords[4], y_coords[4], house_values[4]\n",
    "        \n",
    "ax.plot([unknown_x, nearest_x], [unknown_y, nearest_y], color = \"red\", linewidth = 1)\n",
    "scatter_known(ax)\n",
    "scatter_unknown(ax, label_pred = nearest_label)\n",
    "ax.text(2, 8, f\"Predicted value: {nearest_label}\", fontsize = 5);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908a2eb7",
   "metadata": {},
   "source": [
    "That's great, but can we really trust a single point? It seems problematic that we're using a single point to predict our label - what if that point is an outlier? We'd also expect our prediction to be rather biased, as we're choosing a point to the left.\n",
    "\n",
    "A solution is to use more neighbors - instead of looking at just one point, let's use the **k nearest** labelled neighbors. In the cell below, we'll take the k closest points in parameter space in terms of $L_2$ distance, and take the average label of those points to be our predicted output. Let's first set some value of k:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7091422b",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 9   #Play around with this K value!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b898944",
   "metadata": {},
   "source": [
    "And then visualize what the algorithm is doing; don't worry too much about the specific python implementation right now - just focus on the outputted graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820d1cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_arr = (x_coords - unknown_x) ** 2 + (y_coords - unknown_y) ** 2\n",
    "sorted_inds = np.argsort(l2_arr)\n",
    "sorted_x = x_coords[sorted_inds]\n",
    "sorted_y = y_coords[sorted_inds]\n",
    "sorted_labels = house_values[sorted_inds]\n",
    "predicted_label = np.mean(sorted_labels[:k])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for x, y in zip(sorted_x[:k], sorted_y[:k]):\n",
    "    ax.plot([unknown_x, x], [unknown_y, y], color = \"red\", linewidth = 1)\n",
    "    \n",
    "scatter_known(ax)\n",
    "scatter_unknown(ax, label_pred = predicted_label)\n",
    "ax.text(2, 8, f\"Predicted value: {predicted_label}\", fontsize = 5);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedbbe44",
   "metadata": {},
   "source": [
    "This is the **K-nearest neighbors (KNN)** algorithm! By playing around with the value of k above, we can see how the prediction changes. Intuitively, it makes sense that having too small of a k is not a good idea - our prediction will be highly sensitive to the peculiarities of the surrounding data. Having too large of a k is also not a great idea - we'll be using datapoints with very different features to make predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ed5494",
   "metadata": {},
   "source": [
    "## Questions to Consider"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da572c52",
   "metadata": {},
   "source": [
    "The above example raises some questions:\n",
    "1. How do we determine how many neighbors to look at - that is, what is k?\n",
    "2. Should we weigh neighbors nearby in parameter space equally as neighbors that are far away? If so, how do we know what weighting scheme to use?\n",
    "3. What happens if we don't use $L_2$ distance to find neighbors, but some other distance metric? How do we choose a good metric to use?\n",
    "\n",
    "We'll be exploring answers to these questions in the following section, where we'll be using the full dataset, and the full set of features, to make predictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7217dc07",
   "metadata": {},
   "source": [
    "## The KNN algorithm in depth - data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad40b4b",
   "metadata": {},
   "source": [
    "The first step of data processing is to store the input features, and the target labels, as [numpy arrays](https://numpy.org/devdocs/user/quickstart.html) - when working with machine learning libraries, this is almost always how data is represented for fast processing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66afb64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = housing_data['data'].to_numpy() #Features (income, number of rooms, etc)\n",
    "y = housing_data['target'].to_numpy() #Correct label (actual median house price)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5fea51",
   "metadata": {},
   "source": [
    "We can see that our full dataset consists of 20,640 districts, with 8 features each; in the numpy array, this is represented as a matrix with 20,640 rows, and 8 columns. Our target consists of 20,640 house prices - an array with 20,640 entries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21c3d43",
   "metadata": {},
   "source": [
    "Our goal now is to see how the KNN algorithm performs on new data that it hasn't seen before. For this reason, we'll split our existing dataset into a **training set**, and a **testing set**. \n",
    "* The **training set** constitutes the labelled datapoints.\n",
    "* The **testing set** is where we'll try to predict the labels of the datapoints, using their input features:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152a7ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "test_x = [4.5, 2.5, 7.5]\n",
    "test_y = [6, 6.5, 7.5]\n",
    "ax.scatter([test_x], [test_y], color = \"red\", s = 30) \n",
    "for x_coord, y_coord in zip(test_x, test_y):\n",
    "    ax.annotate(\"???\", (x_coord, y_coord), xytext = (x_coord - 0.1, y_coord + 0.22), color = \"red\");\n",
    "    \n",
    "scatter_known(ax)\n",
    "ax.text(1.8, 8, \"Labelled - Training set. \\n??? - Testing Set\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c9ca2c",
   "metadata": {},
   "source": [
    "To evaluate how KNN is doing, we'll first use KNN to predict the labels of the points in the test set. Then, we'll see how closely those predictions match the actual labels of the testing set - that is, we'll find the **test error**. Scikit-learn, a free machine learning library in python, does a randomized training and testing split for us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d17a94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d25962",
   "metadata": {},
   "source": [
    "Here, we've split the dataset into 80 percent training, and 20 percent testing. \n",
    "\n",
    "Now, remember that we're trying to determine the value of k. We could see one possible way that we could do this - we could try out different values of k until we find one that minimizes the test error. So we deliver the lowest possible test error, and we've optimized our KNN algorithm! Right?\n",
    "\n",
    "The problem with the above approach is that we've cheated - we're *using* the testing set to tune the value of k. Therefore, the test error no longer faithfully represents how well the algorithm performs on new, unseen data - we've already used the testing set to fit k. \n",
    "\n",
    "We can remedy this by further splitting the training set into training and **validation** sets. We'll first try to predict the labels of the *validation* set, and use this to tune k. Once we find a good value of k, we'll use the testing set to get an unbiased evaluation of the KNN performance. \n",
    "\n",
    "![Training, Validation, and Testing Image](https://b1739487.smushcdn.com/1739487/wp-content/uploads/2021/04/train-and-test-1-min-1.png?lossy=0&strip=1&webp=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3161142e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.2, random_state = 42)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ada19e",
   "metadata": {},
   "source": [
    "Some machine learning terminology: K is a **hyperparameter** - a parameter that is not learned by fitting to the training set. Rather, it's a parameter of the model that you set *before* you start using the training set to make predictions. Hyperparameters are chosen by evaluation on the validation set, just as we're doing.\n",
    "\n",
    "The last step of preparing data for KNN is to **scale** the features, so that each feature takes on roughly the same range of values. For instance, if feature X1 has values in the range (0, 100), while feature X2 has values in the range (0, 1), feature X1 will dominate when we're searching for nearest neighbors - the nearest neighbors will almost entirely be determined by the closest values of X1! For this reason, we **scale** X1 and X2 to occupy roughly the same range. \n",
    "\n",
    "We'll use scikit-learn's MinMaxScaler, which linearly scales each feature to have minimum 0 and maximum 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6d1490",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54b59d5",
   "metadata": {},
   "source": [
    "## The Simplest KNN Algorithm - implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f97a2f",
   "metadata": {},
   "source": [
    "We're now ready to actually implement the KNN algorithm. Let's first look at an example where we use 3 neighbors, and see how KNN predicts the first example of the validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8545a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "neigh = KNeighborsRegressor(n_neighbors = 3)\n",
    "neigh.fit(X_train_scaled, y_train)\n",
    "neigh.predict(X_val_scaled[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e061459a",
   "metadata": {},
   "source": [
    "We can predict the labels of every entry in the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31db1985",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_val = neigh.predict(X_val_scaled)\n",
    "pred_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2009cf",
   "metadata": {},
   "source": [
    "And we can see how far this is from the actual labels of the validation set. To do this, we'll use the **mean squared error** between the predicted and acutal labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4bca9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(pred_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9221079c",
   "metadata": {},
   "source": [
    "Now, our goal is to find a value of k that will minimize this validation error: For that, we can simply loop through different values of k. We'll store the errors in the y_errors array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c5b124",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_errors = []\n",
    "for k in range(1, 21):\n",
    "    neigh = KNeighborsRegressor(n_neighbors = k)\n",
    "    neigh.fit(X_train_scaled, y_train)\n",
    "    y_pred = neigh.predict(X_val_scaled)\n",
    "    y_error = mean_squared_error(y_pred, y_val)\n",
    "    y_errors.append(y_error)\n",
    "    \n",
    "y_errors = np.array(y_errors)\n",
    "y_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177462e1",
   "metadata": {},
   "source": [
    "Let's take a look at how the error changes for different values of k:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b43505",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(np.arange(1, 21), y_errors)\n",
    "ax.set_xticks(np.arange(1, 21, 2));\n",
    "ax.set_xlabel(\"K\")\n",
    "ax.set_ylabel(\"Validation Error\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c95c98f",
   "metadata": {},
   "source": [
    "We can see that the error is pretty large for low values of k, and as we increase k, it looks like the error drops until k = 8, where it's about 0.400, after which it starts rising again. This is typical of KNN, and is just like we hypothesized earlier: values of k that are both too small and too large will lead to greater error. Let's confirm that k = 8 is indeed the optimal value, and find the mean squared error on the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66479a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_k = np.argmin(y_errors) + 1 #Add 1 due to the 0-index corresponding to k = 1\n",
    "best_mse = np.min(y_errors)\n",
    "print(f\"Best value of k: {best_k}, best mean squared error: {best_mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959e737d",
   "metadata": {},
   "source": [
    "Now, at this point, you might be asking: how good is this 0.401 number actually? Is this model actually doing anything?\n",
    "\n",
    "One way that we can get a sense of how well the model is performing on the validation set is by comparing the mean squared error with a model that simply predicts the average every time. That is, if the validation set has ground truth labels $y^{(1)}, y^{(2)}, ... y^{(n)}$, a baseline model would just predict $\\bar{y}$ for every point in the set. So then the mean squared error of this baseline model would be:\n",
    "$$MSE = \\frac{1}{n} \\sum_{i}(y^{(i)} - \\bar{y})^2$$\n",
    "\n",
    "If that looks familiar, that's because it's just the variance of the data! So, we'd better hope that the mean squared error from K-nearest neighbors is better than the variance of the validation set - or else we might as well just be guessing the average every time. Let's see what the variance of the validation set is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1894068e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.var(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c072d293",
   "metadata": {},
   "source": [
    "That's comforting! Our simple KNN algorithm is doing quite a bit better than the baseline, at least on the validation set. The question, of course, is whether we can do even better than this. One way that we can improve our performance is by taking into account the distance to the neighbors that we use. That is, when we're deciding the value of some point, we want to consider points close by in parameter space with more weight. We'll explore this in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a33f9a",
   "metadata": {},
   "source": [
    "## Weighted KNN Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2262db4b",
   "metadata": {},
   "source": [
    "Let's formalize the idea above a little bit. In the last section, we came up with a model where we take an unlabelled input, look at the k closest labelled points in parameter space, and take the average label of those points as our prediction. In a weighted scheme, we instead take the **weighted average** of those points to predict the output.\n",
    "\n",
    "Let's start off with a simple weighting scheme: we'll take the weights to be inversely proportional to the distance to the point. Thankfully, scikit-learn already has a built in option for this - we just need to specify the *weights* keyword when defining the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e894dbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "neigh = KNeighborsRegressor(n_neighbors = 3, weights = 'distance')\n",
    "neigh.fit(X_train_scaled, y_train)\n",
    "neigh.predict(X_val_scaled[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01a9d86",
   "metadata": {},
   "source": [
    "Of course, now that we're using a new weighting scheme, we might have a different optimal value of k. Let's run through the same process as before to find the best value of k:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95c8831",
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_errors = []\n",
    "for k in range(1, 21):\n",
    "    neigh = KNeighborsRegressor(n_neighbors = k, weights = 'distance')\n",
    "    neigh.fit(X_train_scaled, y_train)\n",
    "    y_pred = neigh.predict(X_val_scaled)\n",
    "    error = mean_squared_error(y_pred, y_val)\n",
    "    weighted_errors.append(error)\n",
    "    \n",
    "weighted_errors = np.array(weighted_errors)\n",
    "best_k = np.argmin(weighted_errors) + 1 #Add 1 due to the 0-index corresponding to k = 1\n",
    "best_mse = np.min(weighted_errors)\n",
    "print(f\"Best value of k: {best_k}, best mean squared error: {best_mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02e27c0",
   "metadata": {},
   "source": [
    "So it seems that the best value of k is still 8. However, with this weighting scheme, we've managed to improve the mean squared error when compared to the unweighted algorithm from the last section. \n",
    "\n",
    "Of course, we can use other weighting schemes as well - we just want any monotonically decreasing function of distance. Let's see how we would implement a weighting scheme where the weights are inversely proportional to the *square* of the distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dab3d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_function(distances):\n",
    "    return 1 / distances ** 2\n",
    "\n",
    "neigh = KNeighborsRegressor(n_neighbors = 3, weights = weight_function)\n",
    "neigh.fit(X_train_scaled, y_train)\n",
    "neigh.predict(X_val_scaled[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a755f10",
   "metadata": {},
   "source": [
    "To create a custom weight function, as above, we just need to create a function that accepts an array of distances, and returns an array of the same shape containing the weights. We can also rewrite the above using a [lambda function](https://www.w3schools.com/python/python_lambda.asp):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9af6d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "neigh = KNeighborsRegressor(n_neighbors = 3, weights = lambda d: 1/d**2)\n",
    "neigh.fit(X_train_scaled, y_train)\n",
    "neigh.predict(X_val_scaled[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3dee0c3",
   "metadata": {},
   "source": [
    "And find the best k and corresponding mean squared error again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c05e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_errors = []\n",
    "for k in range(1, 21):\n",
    "    neigh = KNeighborsRegressor(n_neighbors = k, weights = lambda d:1/d**2)\n",
    "    neigh.fit(X_train_scaled, y_train)\n",
    "    y_pred = neigh.predict(X_val_scaled)\n",
    "    error = mean_squared_error(y_pred, y_val)\n",
    "    weighted_errors.append(error)\n",
    "    \n",
    "weighted_errors = np.array(weighted_errors)\n",
    "best_k = np.argmin(weighted_errors) + 1 #Add 1 due to the 0-index corresponding to k = 1\n",
    "best_mse = np.min(weighted_errors)\n",
    "print(f\"Best value of k: {best_k}, best mean squared error: {best_mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b604bc7c",
   "metadata": {},
   "source": [
    "Notice that the optimal value of k is no longer 8, and that the mean squared error has dropped again! \n",
    "\n",
    "What happens if we keep going - if we use the inverse distance *cubed* as the weighting, or the inverse distance to the fourth power? It seems rather cumbersome to manually check every case - let's add another loop to do this for us. We'll loop over every value of $\\alpha$, defining the weighting scheme $\\frac{1}{d^\\alpha}$, and find the best $k$ value for each $\\alpha$. Notice that the weighting scheme is another hyperparameter!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ce78fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_alpha = 4\n",
    "max_k = 20\n",
    "weighted_errors = np.zeros((max_alpha, max_k))\n",
    "for alpha in range(1, max_alpha + 1):\n",
    "    print(f\"Running alpha = {alpha}\")\n",
    "    for k in range(1, max_k + 1):\n",
    "        neigh = KNeighborsRegressor(n_neighbors = k, weights = lambda d:1/d**alpha)\n",
    "        neigh.fit(X_train_scaled, y_train)\n",
    "        y_pred = neigh.predict(X_val_scaled)\n",
    "        \n",
    "        error = mean_squared_error(y_pred, y_val)\n",
    "        weighted_errors[alpha - 1, k - 1] = error\n",
    "        \n",
    "best_index = np.unravel_index(np.argmin(weighted_errors), weighted_errors.shape)\n",
    "best_alpha = best_index[0] + 1\n",
    "best_k = best_index[1] + 1\n",
    "best_mse = np.min(weighted_errors)\n",
    "\n",
    "print(f\"best alpha: {best_alpha}, best k: {best_k}, best mse: {best_mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b08cff1",
   "metadata": {},
   "source": [
    "The above is an example of **grid search** for hyperparameters. Grid search can be used in many ML algorithms to find optimal hyperparameters - though you can see how it can quickly become very expensive when there are more hyperparameters! \n",
    "\n",
    "We've managed to get the mean squared error down to 0.3884 by optimizing $\\alpha$ and k. Of course, there are an infinite number of weighting schemes we can use, not necessarily just inverse powers of distance - and it may be that one of them performs better. In practice, it's up to you to decide how much time you want to spend tuning hyperparameters. It may be that squeezing out a little better performance is worth the extra time you take to find that perfect weighting scheme. That's up to the specific problem at hand. \n",
    "\n",
    "For now, though, let's move on and try one more trick to get better performance: using a different distance metric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fc9234",
   "metadata": {},
   "source": [
    "## Changing The Distance Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837e298a",
   "metadata": {},
   "source": [
    "Up until now, we've been using $L_2$ distance in feature space to find nearest neighbors, and to assign weights for the weighted KNN. But we aren't limited to just $L_2$ distance! In general, we can use the [Minkowski distance](https://en.wikipedia.org/wiki/Minkowski_distance) between points X and Y:\n",
    "$$D(X, Y) = \\left(\\sum_{i=1}^{n}|x_i-y_i|^p\\right)^{\\frac{1}{p}}$$\n",
    "\n",
    "Where p is the power parameter. So we can take p to be another hyperparameter to tune! In practice, though, we'll usually just  use p = 1 or p = 2. When p = 1:\n",
    "$$D(X, Y) = \\sum_{i=1}^{n}|x_i-y_i|$$\n",
    "\n",
    "Which is the [Manhattan distance](https://en.wikipedia.org/wiki/Taxicab_geometry), also called the $L_1$ distance. When p = 2, we get:\n",
    "$$D(X, Y) = \\left(\\sum_{i=1}^{n}|x_i-y_i|^2\\right)^{\\frac{1}{2}}$$\n",
    "Which is the familiar $L_2$ distance that we've been using. Let's see how KNN works when we use the Manhattan distance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8eeb647",
   "metadata": {},
   "outputs": [],
   "source": [
    "neigh = KNeighborsRegressor(n_neighbors = 3, p = 1)\n",
    "neigh.fit(X_train_scaled, y_train)\n",
    "neigh.predict(X_val_scaled[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47492b91",
   "metadata": {},
   "source": [
    "Scikit-learn lets us set the Minkowski power parameter p, so we just set it as 1 to use Manhattan distance to do nearest neighbors search, and for the input to the weighting function if there is one. Now, we can repeat grid search over $\\alpha$ and k like we did before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad12285d",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_alpha = 4\n",
    "max_k = 20\n",
    "power_param = 1\n",
    "weighted_errors = np.zeros((max_alpha + 1, max_k))\n",
    "\n",
    "for alpha in range(max_alpha + 1):\n",
    "    print(f\"Running alpha = {alpha}\")\n",
    "    for k in range(1, max_k + 1):\n",
    "        neigh = KNeighborsRegressor(n_neighbors = k, weights = lambda d:1/d**alpha, p = power_param)\n",
    "        neigh.fit(X_train_scaled, y_train)\n",
    "        y_pred = neigh.predict(X_val_scaled)\n",
    "        \n",
    "        error = mean_squared_error(y_pred, y_val)\n",
    "        weighted_errors[alpha, k - 1] = error\n",
    "        \n",
    "best_index = np.unravel_index(np.argmin(weighted_errors), weighted_errors.shape)\n",
    "best_alpha = best_index[0]\n",
    "best_k = best_index[1] + 1\n",
    "best_mse = np.min(weighted_errors)\n",
    "\n",
    "print(f\"best alpha: {best_alpha}, best k: {best_k}, best mse: {best_mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c6c773",
   "metadata": {},
   "source": [
    "Notice that I'm searching over $\\alpha = 0$ now, which is the unweighted KNN case. We can see that the best mean squared error has dropped down to 0.358! \n",
    "\n",
    "Now, in principle, we could search over other values of p by adding *another* outer loop. However, we won't do that here, as this is (1) generally not very useful, or commonly used, and (2) quite slow on scikit-learn: not only do we have another dimension to grid search, but using p>2 is actually very slow on scikit-learn. If you're interested, however, try playing around with different values of p by changing the power_param in the above cell!\n",
    "\n",
    "In the meantime, we'll proceed with our optimal hyperparameters: Manhattan distance, a weighting scheme of $\\frac{1}{d^3}$, and $k = 17$. We've tuned our hyperperparameters with the validation set - now let's see how they work on the *test* set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086785b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "neigh = KNeighborsRegressor(n_neighbors = best_k, weights = lambda d:1/d**best_alpha, p = 1)\n",
    "neigh.fit(X_train_scaled, y_train)\n",
    "y_pred = neigh.predict(X_test_scaled)\n",
    "mean_squared_error(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e90ebf",
   "metadata": {},
   "source": [
    "Compared to the variance of the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014691c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.var(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d2e696",
   "metadata": {},
   "source": [
    "We can be confident that our KNN algorithm is able to *generalize* to data that it hasn't seen before, and that we didn't use to train or tune hyperparameters!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b56cd5",
   "metadata": {},
   "source": [
    "## A Condensed Matter Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7528da5",
   "metadata": {},
   "source": [
    "Now, let's take everything we've learned, and apply it to something more relevant to physics than house prices - predicting spectral functions in a model of high temperature superconductivity, a project that I recently worked on. Let's start by loading the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35cd293",
   "metadata": {},
   "outputs": [],
   "source": [
    "spectral_data = np.load(\"spectral_data.npz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de13ab6e",
   "metadata": {},
   "source": [
    "About the dataset:\n",
    "- The **input features** are three Hamiltonian parameters: $t'$, $t''$, and $J$. The $t'$ and $t''$ describe electron hopping in a lattice, while $J$ describes spin interactions between neighboring sites in the lattice. \n",
    "- The **label** is the quasiparticle spectral function derived from those parameters, which was obtained with computational physics. If you aren't familiar with spectral functions, they describe the allowed states of a system, and are an important experimental observable. \n",
    "\n",
    "Let's get the Hamiltonian parameters and spectral functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922a9686",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = spectral_data['params']\n",
    "y = spectral_data['dos']\n",
    "\n",
    "print(f\"Parameter Shape: {X.shape}\")\n",
    "print(f\"Spectral Functions Shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259efe50",
   "metadata": {},
   "source": [
    "We can see that we have 17,576 data points, each consisting of a set of 3 Hamiltonian parameters, and the corresponding spectral function. Let's take a look at what the first spectral function looks like: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729b3001",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1, t2, j = X[0]\n",
    "dos = y[0]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(np.linspace(-6, 6, 301), dos, color = \"black\")\n",
    "ax.set_xlabel(\"$\\omega/t$\")\n",
    "ax.set_title(f\"$t'={t1}, t''={t2}, J={j}$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d94aa2",
   "metadata": {},
   "source": [
    "To turn the continuous spectral function into something we can work with, we've sampled the spectral function at 301 points, producing a 301 dimensional vector. So, each datapoint consists of 3 dimensional input, and a 301 dimensional output.\n",
    "\n",
    "Here's the problem we want to solve: We have a set of datapoints, consisting of Hamiltonian parameters and their corresponding spectral functions. Can we use KNN to predict the resulting spectral function of a *new* set of Hamiltonian parameters? If we can, we wouldn't have to run the physics computation using the parameters - a huge improvement in computational time!\n",
    "\n",
    "Notice that there's a new twist in this problem, compared to the house prices: our labels are now *vectors*, instead of scalars. How will we use KNN, which involves taking the mean of the labels? No matter - we can just take the element-wise mean, as below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471ddcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(np.linspace(-6, 6, 301), y[0], color = \"blue\", label = \"DOS 1\", linewidth = 1)\n",
    "ax.plot(np.linspace(-6, 6, 301), y[500], color = \"red\", label = \"DOS 2\", linewidth = 1)\n",
    "ax.plot(np.linspace(-6, 6, 301), (y[0] + y[500]) / 2, color = \"purple\", label = \"Mean\", linewidth = 1)\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"$\\omega/t$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c99973a",
   "metadata": {},
   "source": [
    "So it's possible, but will taking averages like this really produce good spectral functions? Let's try it out. First, we'll divide the dataset into training, validation, and testing sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f794de",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.25, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c36b4f",
   "metadata": {},
   "source": [
    "And scale the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fde3f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37898216",
   "metadata": {},
   "source": [
    "And then we'll run grid search on the KNN hyperparameters to minimize error on the validation set. Let's start by doing so with the Manhattan distance, and trying out weighting schemes with $\\frac{1}{d^\\alpha}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd52cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_alpha = 5\n",
    "max_k = 20\n",
    "power_param = 1\n",
    "weighted_errors = np.zeros((max_alpha + 1, max_k))\n",
    "\n",
    "for alpha in range(max_alpha + 1):\n",
    "    print(f\"Running alpha = {alpha}\")\n",
    "    for k in range(1, max_k + 1):\n",
    "        neigh = KNeighborsRegressor(n_neighbors = k, weights = lambda d:1/d**alpha, p = power_param)\n",
    "        neigh.fit(X_train_scaled, y_train)\n",
    "        y_pred = neigh.predict(X_val_scaled)\n",
    "        \n",
    "        error = mean_squared_error(y_pred, y_val)\n",
    "        weighted_errors[alpha, k - 1] = error\n",
    "        \n",
    "best_index = np.unravel_index(np.argmin(weighted_errors), weighted_errors.shape)\n",
    "best_alpha = best_index[0]\n",
    "best_k = best_index[1] + 1\n",
    "best_mse = np.min(weighted_errors)\n",
    "\n",
    "print(f\"best alpha: {best_alpha}, best k: {best_k}, best mse: {best_mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216fb68f",
   "metadata": {},
   "source": [
    "And let's try with the $L_2$ distance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218ec17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_alpha = 5\n",
    "max_k = 20\n",
    "power_param = 2\n",
    "weighted_errors = np.zeros((max_alpha + 1, max_k))\n",
    "\n",
    "for alpha in range(max_alpha + 1):\n",
    "    print(f\"Running alpha = {alpha}\")\n",
    "    for k in range(1, max_k + 1):\n",
    "        neigh = KNeighborsRegressor(n_neighbors = k, weights = lambda d:1/d**alpha, p = power_param)\n",
    "        neigh.fit(X_train_scaled, y_train)\n",
    "        y_pred = neigh.predict(X_val_scaled)\n",
    "        \n",
    "        error = mean_squared_error(y_pred, y_val)\n",
    "        weighted_errors[alpha, k - 1] = error\n",
    "        \n",
    "best_index = np.unravel_index(np.argmin(weighted_errors), weighted_errors.shape)\n",
    "best_alpha = best_index[0]\n",
    "best_k = best_index[1] + 1\n",
    "best_mse = np.min(weighted_errors)\n",
    "\n",
    "print(f\"best alpha: {best_alpha}, best k: {best_k}, best mse: {best_mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ee2a18",
   "metadata": {},
   "source": [
    "So it seems that $L_2$ distance is better. We've gotten an optimized KNN algorithm - now let's see how it does on the testing set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7e5672",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_knn = KNeighborsRegressor(n_neighbors = best_k, weights = lambda d:1/d**best_alpha, p = 2)\n",
    "best_knn.fit(X_train_scaled, y_train)\n",
    "y_pred = best_knn.predict(X_test_scaled)\n",
    "mean_squared_error(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473b27d1",
   "metadata": {},
   "source": [
    "But how do the predicted spectral functions actually look in comparison with the ground truths? One way of seeing this is by seeing how the worst predicted spectral functions (in terms of mean squared error) look. If even these spectral functions look pretty close to the ground truth, we can be confident the spectral functions in the whole test set look close. \n",
    "\n",
    "Let's order the predicted spectral functions in the test set by mean squared error compared to the ground truth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0740a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_list = np.mean((y_pred - y_test) ** 2, axis = 1)\n",
    "sorted_inds = np.argsort(mse_list)[::-1]\n",
    "sorted_pred = y_pred[sorted_inds]\n",
    "sorted_truth = y_test[sorted_inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff548f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(np.linspace(-6, 6, 301), sorted_truth[0], color = \"black\", linewidth = 3, label = \"Ground Truth\")\n",
    "ax.plot(np.linspace(-6, 6, 301), sorted_pred[0], color = \"red\", linewidth = 1, label = \"KNN Predicted\")\n",
    "ax.set_title(\"Worst mean squared error in test set\")\n",
    "\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"$\\omega/t$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ca73ae",
   "metadata": {},
   "source": [
    "And let's take a look at what the lower quartile of performance looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6ad584",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "test_size = len(sorted_truth)\n",
    "ax.plot(np.linspace(-6, 6, 301), sorted_truth[test_size // 4], color = \"black\", linewidth = 3, label = \"Ground Truth\")\n",
    "ax.plot(np.linspace(-6, 6, 301), sorted_pred[test_size // 4], color = \"red\", linewidth = 1, label = \"KNN Predicted\")\n",
    "ax.set_title(\"Lower quartile MSE in test set\")\n",
    "\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"$\\omega/t$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e70a2f",
   "metadata": {},
   "source": [
    "Pretty good! Even for the very worst example in the test set, the peak positions and widths are well predicted, and by the time we get to the lower quartile, the spectral functions almost match exactly. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff88080",
   "metadata": {},
   "source": [
    "## Limitations of KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48e8f77",
   "metadata": {},
   "source": [
    "We've seen how KNN can be useful in making predictions, despite its simplicity; however, we also need to be careful of its limitations! Let's take a look at some of the limitations of KNN:\n",
    "- **Irrelevant features** can significantly impact the performance of the algorithm. An irrelevant feature is one that has no predictive power for the label of a point. As KNN assumes that points close in feature space have similar labels, such irrelevant features can mess up the results!\n",
    "- **KNN can be slow for very large datasets**, as labelling each new point requires looking at training data.\n",
    "- KNN is especially impacted by **high dimensional datasets** - datasets with a lot of features. This is because the distance the distance metric breaks down in high dimensions: that is, distances between most points become almost the same in high dimensional parameter space. Clearly, this is an issue for KNN! This is known as the **curse of dimensionality** - [this lecture](https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote02_kNN.html) discusses this in more depth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b004b72f",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6c9f95",
   "metadata": {},
   "source": [
    "KNN regression is a simple algorithm - you just look at the closest points in parameter space, and take their weighted average. However, we've seen how KNN can nonetheless be quite effective in predicting the labels of new points - even applying it to a problem of condensed matter physics. There are times when applying KNN produces results that are good enough - which is great, because then you don't have to use more complicated machine learning algorithms! In other cases, KNN can provide a useful benchmark for these other, more sophisticated machine learning algorithms. As with any other ML algorithm, KNN's success is dependent on that particular dataset. Go forth, and may KNN solve (some) of your problems!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
