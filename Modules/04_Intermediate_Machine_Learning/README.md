<div align=center>

# Intermediate machine learning

</div>

Despite the ubiquitousness of modern-day large language models and other neural networks, a tremendous amount can be accomplished using simpler models. Not only this, but a significant amount of key concepts in machine learning can be more easily demonstrated using approaches which "leave aside" the complexities of neural networks and advanced optimizers. In this set of modules, we present multiple walkthroughs of non-neural network-based machine learning methods.

1. [K-nearest neighbor regression](https://colab.research.google.com/github/matthewcarbone/Bootcamp/blob/master/Modules/04_Intermediate_Machine_Learning/01_KNN.ipynb)
2. [Support vector machines](https://colab.research.google.com/github/matthewcarbone/Bootcamp/blob/master/Modules/04_Intermediate_Machine_Learning/02_SVM.ipynb)
3. [Random forests](https://colab.research.google.com/github/matthewcarbone/Bootcamp/blob/master/Modules/04_Intermediate_Machine_Learning/03_RF.ipynb)


# üôè Acknowledgements

A huge amount of the material here was written by [Jackson Lee](https://github.com/JackieLee23/), who at the time of this commit is an up-and-coming graduate student at Columbia University.
