{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we'll go over the Support Vector Machine (SVM) classification algorithm. The SVM algorithm is a supervised learning algorithm, meaning that we train the SVM on a set of labelled data, which then allows the SVM to predict the labels of future, unlabelled data. Depending on the dataset, SVMs often allow for easier training and more accurate results when compared to other algorithms. By the end of this tutorial, we'll have covered: \n",
    "1. The intuition behind what SVMs are really doing.\n",
    "2. The math behind how SVMs train\n",
    "3. How SVMs can be extended to a wide variety of nonlinear classification problems. \n",
    "4. The practicalities of using SVMs, including processing data and tuning SVMs\n",
    "5. A fully coded, real world example\n",
    "\n",
    "First, if running a Google Colab instance, you'll need to pull the data manually:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -o mock_cancer_dataset.csv https://raw.githubusercontent.com/JackieLee23/SVM_tutorial/main/mock_cancer_dataset.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first import some libraries that we'll need first, and set some defaults for plotting that we'll need later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "from matplotlib.widgets import Slider\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "plt.rcParams[\"figure.dpi\"] = 250\n",
    "plt.rcParams[\"figure.figsize\"] = [3, 2]\n",
    "plt.rcParams[\"font.size\"] = 5\n",
    "plt.rcParams[\"lines.markersize\"] = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction: Linear Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start with a mock dataset - a health dataset consisting of data for a bunch of different tumors. We'll actually be working with a real cancer dataset later in this tuturial - but for now, let's work with a mock dataset that I've generated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mock_cancer_df = pd.read_csv(\"mock_cancer_dataset.csv\")\n",
    "mock_cancer_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the first 2 columns are the **features** of the tumors - namely, the area of the tumor, and the perimeter of the tumor. Meanwhile, the last column is the **target** that we're trying to predict - whether or not the tumor is malignant or benign. The table above is a [pandas](https://pandas.pydata.org/docs/) dataframe - essentially a table with each row being a datapoint, and the columns being different features. Let's first extract this data into [NumPy](https://numpy.org/) arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benign_features = mock_cancer_df[mock_cancer_df[\"Diagnosis\"] == \"Benign\"].iloc[:, :2].to_numpy()\n",
    "mal_features = mock_cancer_df[mock_cancer_df[\"Diagnosis\"] == \"Malignant\"].iloc[:, :2].to_numpy()\n",
    "feature_1, feature_2 = mock_cancer_df.columns[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NumPy arrays let us work with the dataset with speed and ease. Let's use the plotting library [matplotlib](https://matplotlib.org/stable/tutorials/introductory/quick_start.html#sphx-glr-tutorials-introductory-quick-start-py) to plot this data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding = 5\n",
    "x_lims = np.min(benign_features[:, 0])  - padding, np.max(mal_features[:, 0]) + padding\n",
    "y_lims = np.min(benign_features[:, 1]) - padding, np.max(mal_features[:, 1]) + padding\n",
    "x_lims, y_lims = np.array(x_lims), np.array(y_lims)\n",
    "\n",
    "def scatter_training_set(ax):\n",
    "    ax.set_xlim(x_lims)\n",
    "    ax.set_ylim(y_lims)\n",
    "    ax.set_xlabel(feature_1)\n",
    "    ax.set_ylabel(feature_2)\n",
    "    return (ax.scatter(benign_features[:, 0], benign_features[:, 1], c = \"green\", marker = \"o\", label = \"Benign\"),\n",
    "            ax.scatter(mal_features[:, 0], mal_features[:, 1], c = \"red\", marker = \"x\", label = \"Malignant\"))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "scatter_training_set(ax)\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now suppose we're given a new datapoint, for which we don't know whether the point is malignant or benign. The **supervised classification** problem is to use our current labelled dataset to predict the new, unlabelled point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_x, unknown_y = 65, 30\n",
    "unknown_scatter = ax.scatter(unknown_x, unknown_y, marker = 'o', c = \"black\")\n",
    "unknown_annotate = ax.annotate(\"???\", (unknown_x, unknown_y), xytext = (unknown_x + 2, unknown_y), color = \"red\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVM approach is to make a **separating hyperplane**: a plane that divides the benign and malignant points of our labelled dataset in feature space. In our simple 2d case, that separating hyperplane will just be a line dividing the benign and malignant points. And to make a prediction for the new unlabelled point, we just see what side of the line the point falls on!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_slope = -0.5\n",
    "initial_intercept = 70\n",
    "y_line = initial_slope * x_lims + initial_intercept\n",
    "\n",
    "text_benign = ax.text(65, 20, \"Predicted Benign\")\n",
    "text_malignant = ax.text(55, 50, \"Predicted Malignant\")\n",
    "line, = ax.plot(x_lims, y_line, lw=2, \n",
    "                color='blue', label='Hyperplane')\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that's great, but we immediately see a problem - there are many different choices of the separating line! You can toggle the sliders in the cell below and see that there are lots of different lines that separate the benign and malignant points - but they'll give different predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "fig, ax = plt.subplots()\n",
    "fig.subplots_adjust(left=0.25, bottom=0.25)\n",
    "ax_slope = fig.add_axes([0.25, 0.05, 0.65, 0.03], \n",
    "                facecolor='lightgoldenrodyellow')\n",
    "ax_intercept = fig.add_axes([0.1, 0.25, 0.03, 0.65], \n",
    "                        facecolor='lightgoldenrodyellow')\n",
    "slider_slope = Slider(ax_slope, 'Slope', -4.0, 0.0,\n",
    "                    valinit=initial_slope)\n",
    "slider_intercept = Slider(ax_intercept, 'Y-Intercept', 50.0, 200.0, \n",
    "                        valinit=initial_intercept, orientation = 'vertical')\n",
    "scatter_training_set(ax)\n",
    "line, = ax.plot(x_lims, y_line, lw=2, \n",
    "                color='blue', label='Hyperplane')\n",
    "\n",
    "def set_line_data():\n",
    "    slope = slider_slope.val\n",
    "    intercept = slider_intercept.val\n",
    "    line.set_ydata(slope * x_lims + intercept)\n",
    "\n",
    "def update(val):\n",
    "    set_line_data()\n",
    "    fig.canvas.draw_idle()\n",
    "\n",
    "slider_slope.on_changed(update)\n",
    "slider_intercept.on_changed(update)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So which separating line do we choose?\n",
    "An intuitive solution to this problem is to look for the line that maximizes the distance from the line to the nearest labelled datapoint. This distance is called the **margin**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_margin(slack_index = -1):\n",
    "    slope = slider_slope.val\n",
    "    intercept = slider_intercept.val\n",
    "    combined_data = np.concatenate((benign_features, mal_features))\n",
    "    w = np.array([-slope, 1]) / np.sqrt((slope ** 2 + 1))\n",
    "    scalar_proj = np.sum(combined_data[slack_index + 1:] * w, axis = 1) - intercept * w[1]\n",
    "    return np.min(np.abs(scalar_proj))\n",
    "\n",
    "margin_line_upper, = ax.plot(x_lims, [0, 0], lw=1, color='black', label='Hyperplane', linestyle = '--')\n",
    "margin_line_lower, = ax.plot(x_lims, [0, 0], lw=1, color='black', label='Hyperplane', linestyle = '--')\n",
    "\n",
    "def set_margin_lines(margin):\n",
    "    margin_shift = margin * np.sqrt(slider_slope.val**2 + 1)\n",
    "    margin_line_upper.set_ydata(line.get_ydata() + margin_shift)\n",
    "    margin_line_lower.set_ydata(line.get_ydata() - margin_shift)\n",
    "\n",
    "def update(val):\n",
    "    set_line_data()\n",
    "    margin = get_margin()\n",
    "    set_margin_lines(margin)\n",
    "    ax.set_title(f\"Margin: {margin:.2f}\")\n",
    "    fig.canvas.draw_idle()\n",
    "\n",
    "slider_slope.on_changed(update)\n",
    "slider_intercept.on_changed(update)\n",
    "update(0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try to formalize what we've been doing with a bit of math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All the math you need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the concepts above can be formulated using **projections**. The **scalar projection** of vector **a** onto vector **b** is the component of **a** in the direction of **b**: \n",
    "\n",
    "![image](https://github.com/JackieLee23/SVM_tutorial/blob/main/Figures/positive_projection.png?raw=true)\n",
    "\n",
    "If **a** points in the opposite direction of **b**, the scalar projection is negative:\n",
    "\n",
    "![image](https://github.com/JackieLee23/SVM_tutorial/blob/main/Figures/negative_projection.png?raw=true)\n",
    "\n",
    "We can compute the scalar projection as:\n",
    "$$\\frac{\\mathbf{a} \\cdot \\mathbf{b}}{||\\mathbf{b}||}$$\n",
    "\n",
    "Where $||\\mathbf{b}||$ is the length of vector **b**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does that help us with classification? Well, let's say we have a point with coordinates $\\mathbf{x} = (x_1, x_2)$, and a hyperplane, and we're trying to classify the point:\n",
    "\\\n",
    "\\\n",
    "<img src=\"https://github.com/JackieLee23/SVM_tutorial/blob/main/Figures/classification_1.png?raw=true\" alt=\"drawing\" width=\"800\"/>\n",
    "\n",
    "Now, we can of course visually classify the point as above or below the hyperplane - but how can we do so mathematically? One way is to draw the vector **w**, called the **weight vector**, which is **perpendicular** to our hyperplane:\n",
    "\\\n",
    "\\\n",
    "<img src=\"https://github.com/JackieLee23/SVM_tutorial/blob/main/Figures/classification_2.png?raw=true\" alt=\"drawing\" width=\"800\"/>\n",
    "\n",
    "And then we can take the scalar projection of **x** onto **w**.\n",
    "\\\n",
    "\\\n",
    "<img src=\"https://github.com/JackieLee23/SVM_tutorial/blob/main/Figures/classification_3.png?raw=true\" alt=\"drawing\" width=\"800\"/>\n",
    "\n",
    "And we can see that if the scalar projection of **x** onto **w** is large enough, we can classify **x** as being on the side of the hyperplane that **w** points to - in this case, above the hyperplane. Finally, let's call the distance from the origin along **w** to the hyperplane $b$:\n",
    "\\\n",
    "\\\n",
    "<img src=\"https://github.com/JackieLee23/SVM_tutorial/blob/main/Figures/classification_4.png?raw=true\" alt=\"drawing\" width=\"800\"/>\n",
    "\n",
    "And we see that the signed distance from $\\mathbf{x}$ to our hyperplane is going to be:\n",
    "\n",
    "$$\\frac{\\mathbf{x} \\cdot \\mathbf{w}}{||\\mathbf{w}||} - d$$\n",
    "\n",
    "This signed distance will be *positive* if it's above the hyperplane, and *negative* if it's below. In other words, taking the sign of $$\\frac{\\mathbf{x} \\cdot \\mathbf{w}}{||\\mathbf{w}||} - d$$ gives us the classification of the point!\n",
    "\n",
    "To make this a bit neater, we'll define a variable $b$, called the **bias**, as $d = \\frac{-b}{||w||}$. Then \n",
    "\n",
    "$$\\text{sgn}(\\frac{\\mathbf{x} \\cdot \\mathbf{w}}{||\\mathbf{w}||} - d) = \\text{sgn}(\\mathbf{x} \\cdot \\mathbf{w} + b) $$\n",
    "\n",
    "Let's review what we just did: in 2 dimensional feature space, we had a 1 dimensional separating hyperplane (a line), and we found that by drawing a 2 dimensional weight vector $\\mathbf{w}$, and finding bias $b$, we could compute what side of the hyperplane a point $\\mathbf{x}$ falls on. In practice, it's a lot easier to *start* with $\\mathbf{w}$ and $b$, and let those define a hyperplane! This also lets us generalize to higher dimensions. In $n$ dimensional space, we'll have an $n$ dimensional weight vector, and a bias $b$, which lets us define an $n-1$ dimensional hyperplane. And we can instantly tell if a point is above or below the hyperplane by computing the sign of $\\mathbf{x} \\cdot \\mathbf{w} + b$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you were paying attention, you might see that computing the margin for a hyperplane is also really easy! Remember, the margin is the distance from the closest point to the hyperplane. But we already found the distance from $\\mathbf{x}$ to the hyperplane is $$|\\frac{\\mathbf{x} \\cdot \\mathbf{w}}{||\\mathbf{w}||} - d|$$ or $$|\\frac{\\mathbf{x} \\cdot \\mathbf{w}}{||\\mathbf{w}||} + \\frac{b}{||w||}|$$\n",
    "\n",
    "And so the **margin** is just the smallest value of $|\\frac{\\mathbf{x} \\cdot \\mathbf{w}}{||\\mathbf{w}||} + \\frac{b}{||w||}|$ in our labelled dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding some slack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the point of the support vector machine is to find a separating hyperplane that divides the labelled datapoints according to their labels, while also maximizing the margin - this quantity $|\\frac{\\mathbf{x} \\cdot \\mathbf{w}}{||\\mathbf{w}||} + \\frac{b}{||w||}|$. And if we want to predict the label of some new datapoint, we calculate $\\text{sgn}(\\mathbf{x} \\cdot \\mathbf{w} + b)$. But what if we have some rogue datapoints? Let's consider the example below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_index = 0\n",
    "outlier_benign = (70, 35)\n",
    "benign_features[outlier_index] = outlier_benign\n",
    "scatter = scatter_training_set(ax)\n",
    "update(0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that this outlier datapoint is really getting in the way of us having a large margin. In fact, in order for us to maximize the margin, we need to put the hyperplane in between the outlier and the red datapoints - and this doesn't really feel like the correct hyperplane that will actually generalize when we have new data! We need a way to maximize the margin to all of the other points, while taking into account what we're doing with the outlier. \n",
    "\\\n",
    "\\\n",
    "The SVM approach is to add what's called **slack**. Essentially, we allow some of our datapoints to fall *inside* of the margin! And the *slack* quantifies exactly how far inside of the margin the point falls. Let's take a look at what this means:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_dist_to_outlier(outlier_index):\n",
    "    w = np.array([-slider_slope.val, 1]) / np.sqrt((slider_slope.val ** 2 + 1))\n",
    "    return np.sum(benign_features[outlier_index] * w) - slider_intercept.val * w[1] \n",
    "\n",
    "def update(val):\n",
    "    set_line_data()\n",
    "    margin = get_margin(slack_index = outlier_index)\n",
    "    set_margin_lines(margin)\n",
    "    slack = max(margin + find_dist_to_outlier(outlier_index), 0)\n",
    "    ax.set_title(f\"Margin: {margin:.2f}, slack: {slack:.2f}\")\n",
    "    fig.canvas.draw_idle()\n",
    "\n",
    "slider_slope.on_changed(update)\n",
    "slider_intercept.on_changed(update)\n",
    "update(0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the way, this means that we have to modify what we mean by the margin - it no longer strictly means the distance from the hyperplane to the closest datapoint. It now means the distance from the hyperplane to the closest datapoint *without slack*. This approach also lets us apply SVMs even when the data might not be linearly seperable: we allow the data that is messing up the seperability to have slack. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_index = 0\n",
    "outlier_benign = (100, 60)\n",
    "benign_features[outlier_index] = outlier_benign\n",
    "%matplotlib widget\n",
    "scatter[0].remove()\n",
    "scatter = scatter_training_set(ax)\n",
    "update(0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that by allowing the outlier to have slack, we can focus on maximizing the margin to the other data points, and create a more representative hyperplane! If we give the outlier enough slack, we can even allow the outlier to fall on the wrong side of the hyperplane!\n",
    "\\\n",
    "\\\n",
    "Of course, we can't just aribtrarily allow slack - if we did, then we wouldn't have to correctly divide any data! For this reason, SVMs control a tradeoff between slack and maximizing the margin. SVMs work by finding an optimal $\\mathbf{w}$ and $b$ that allows us to have large margins on most of the data, while not allowing for too much slack. Essentially, SVMs try to minimize the **cost function**:\n",
    "$$L(w, b) = [\\text{penalty for having small margins}] + C * [\\text{penalty for having slack}] $$\n",
    "\n",
    "This parameter $C$ is super important - it controls how much we care about penalizing slack. Let's consider 2 scenarios:\n",
    "1. $C$ is super small - close to 0. Then, we really don't penalize slack, and we essentially ignore the points that we misclassify or that are close to the hyperplane. Our slack can be as large as we want!\n",
    "\n",
    "<img src=\"https://github.com/JackieLee23/SVM_tutorial/blob/main/Figures/large_margin_large_slack.png?raw=true\" alt=\"drawing\" width=\"600\"/>\n",
    "\n",
    "2. $C$ is super large. Then, we *really* penalize slack, and so we need there to be almost no slack. That means that the SVM will be sensitive to all points in the training data - even outliers. \n",
    "\n",
    "<img src=\"https://github.com/JackieLee23/SVM_tutorial/blob/main/Figures/small_margin_small_slack.png?raw=true\" alt=\"drawing\" width=\"600\"/>\n",
    "\n",
    "$C$ is what is called a *hyperparameter* - a parameter that we don't learn from the training data, but that we need to set before we start training!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Optional Math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following section, we'll be formalizing the intuition that we just developed about slack variables. This section is a bit more technical and is optional, but is what you'll often see in machine learning lecture notes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say that we have a dataset of labelled examples, $S = \\{(\\mathbf{x}^{(i)}, y^{(i)}); i = 1,...,m\\}$, with $\\mathbf{x}^{(i)}$ being the features of the ith datapoint, and $y^{(i)}$ being the label of the ith datapoint, which is either $y^{(i)} = 1$ or $y^{(i)} = -1$.\n",
    "\n",
    "Recall that the *signed* distance from $\\mathbf{x}^{(i)}$ to the hyperplane is $\\frac{\\mathbf{w} \\cdot \\mathbf{x}^{(i)}}{||\\mathbf{w}||} - d = \\frac{\\mathbf{w} \\cdot \\mathbf{x}^{(i)}}{||\\mathbf{w}||} + \\frac{b}{||w||}$. If we have a linearly seperable dataset, that means that the sign of the signed distance will match $y^{(i)}$: negative if $y^{(i)} = -1$, positive if $y^{(i)} = 1$. That means that the actual distance - which is always positive - will be:\n",
    "\n",
    "$$y^{(i)} * (\\frac{\\mathbf{w} \\cdot \\mathbf{x}^{(i)}}{||\\mathbf{w}||} + \\frac{b}{||\\mathbf{w}||})$$\n",
    "\n",
    "<img src=\"https://github.com/JackieLee23/SVM_tutorial/blob/main/Figures/classification_4.png?raw=true\" alt=\"drawing\" width=\"800\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can neatly write the margin as:\n",
    "\n",
    "$$\\gamma = \\min_{i=1\\dots m} y^{(i)} * (\\frac{\\mathbf{x}^{(i)} \\cdot \\mathbf{w}}{||\\mathbf{w}||} + \\frac{b}{\\mathbf{||w||}})$$\n",
    "\n",
    "Remember, the goal of the SVM, when we don't allow for slack, is to choose $\\mathbf{w}$ and $b$ that will maximize the margin - that is:\n",
    "\n",
    "$$\\text{arg}\\max_{\\mathbf{w}, b} \\gamma$$\n",
    "\n",
    "Now, you might notice that we actually have freedom to scale $\\mathbf{w}$ and $b$. If we replace $\\mathbf{w}$ with $2 * \\mathbf{w}$, and $b$ with $2b$, we'll end up with the same value of $\\gamma$! Intuitively, scaling $w$ and $b$ doesn't actually change the separating hyperplane. For this reason, for a given hyperplane, and linearly separable data, we can always set:\n",
    "\n",
    "$$\\gamma * ||\\mathbf{w}|| = 1$$\n",
    "\n",
    "And so:\n",
    "\n",
    "$$y^{(i)} * (\\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b) \\geq 1$$\n",
    "\n",
    "And so then for any hyperplane, $\\gamma = \\frac{1}{||\\mathbf{w}||}$, and so our optimization objective is:\n",
    "\n",
    "$$\\text{arg}\\max_{w, b} \\gamma = \\text{arg}\\max_{w, b} \\frac{1}{||\\mathbf{w}||} = \\text{arg}\\min_{w, b} ||\\mathbf{w}||^2$$\n",
    "$$\\text{S.T.   } \\mathbf{y^{(i)}} * (\\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b) \\geq 1 \\text{    for all i}$$\n",
    "\n",
    "We've transformed our optimization problem into a much easier optimization problem, subject to a constraint (the second line). We just have to find the smallest length $w$ that satisfies the constraint! It turns out that this form of the optimization objective is much easier to solve using standard off-the-shelf algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how our final expression intuitively makes sense here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(val):\n",
    "    set_line_data()\n",
    "    margin = get_margin()\n",
    "    set_margin_lines(margin)\n",
    "    ax.set_title(f\"$\\gamma = ${margin:.2f}, need $\\gamma * ||\\mathbf{{w}}|| = 1$, so $||\\mathbf{{w}}|| = {1/margin:.3f}$\")\n",
    "    fig.canvas.draw_idle()\n",
    "\n",
    "scatter[0].remove()\n",
    "slider_slope.on_changed(update)\n",
    "slider_intercept.on_changed(update)\n",
    "update(0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that when the hyperplane has a smaller margin, we need a correspondingly larger $\\mathbf{w}$ to satisfy the constraint! And by minimizing $\\mathbf{w}$, we will indeed find the hyperplane with largest margin. \n",
    "\\\n",
    "\\\n",
    "What about slack? Well, remember slack means that we allow some points to fall inside the margin. Mathematically, then, we just allow the point to *violate* that constraint by an amount $\\xi_i$, so that we only have to satisfy $y^{(i)} * (\\mathbf{w} \\cdot \\mathbf{x} + b) = 1 - \\xi_i$. Of course, the slack $\\xi_i$ cannot be negative - if it were, the constraint would be satisfied, and we'd have no need for a slack variable! And so our complete SVM minimization objective is:\n",
    "\n",
    "$$\\text{arg}\\min_{w, b} ||\\mathbf{w}||^2 + \\sum_{i=1}^{m}\\xi_i$$\n",
    "\n",
    "$$\\text{S.T.   } \\mathbf{y^{(i)}} * (\\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b) \\geq 1 -\\xi_i \\text{    for all i}$$\n",
    "\n",
    "$$\\xi_i \\geq 0 \\text{    for all i}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you might be wondering how the name \"Support Vector Machine\" comes in. What are support vectors? Well, intuitively, you might realize that the only datapoints that actually determine the value of $\\mathbf{w}$ are those that fall on or within the margin - that is, those that contribute to the slack, or those that determine the size of the margin. \n",
    "\n",
    "<img src=\"https://github.com/JackieLee23/SVM_tutorial/blob/main/Figures/large_margin_large_slack.png?raw=true\" alt=\"drawing\" width=\"600\"/>\n",
    "\n",
    "In fact, it turns out that we can always write the optimal set of weights as a linear combination of those points:\n",
    "\n",
    "$$w^* = \\sum_{i=1}^{N} \\alpha_i y_i \\mathbf{x}_i$$\n",
    "\n",
    "$$\\alpha_i \\neq 0 \\text{ only for points where } y_i (w^* \\cdot \\mathbf{x}_i + b) \\leq 1$$ \n",
    "\n",
    "That is, the optimal $w$ is a linear combination of datapoints, and that linear combination only consists of points that exactly fall on the margin, or that violate the margin (and thus need slack). These points are called the **support vectors**. They're very useful, since training an SVM involves computing many dot products with $\\mathbf{w}$. However, writing $\\mathbf{w}$ in terms of a linear combination of support vectors means that we just have to compute the sum of dot products with the support vectors:\n",
    "\n",
    "$$\\text{sgn}(\\mathbf{w} \\cdot \\mathbf{x} + b) = \\text{sgn}(\\sum_{i=1}^{N} \\alpha_i y_i (\\mathbf{x}_i \\cdot \\mathbf{x}) + b)$$\n",
    "\n",
    "And **training** an SVM (which is done with off the shelf packages), becomes a matter of finding the support vectors and their corresponding $\\alpha_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a second to review what we've covered so far:\n",
    "1. SVMs are given a set of data, labelled by their corresponding classes. \n",
    "2. SVMs draw a hyperplane to separate the data by their class. In order to predict the class of a new datapoint, you just look at what side of the hyperplane that point falls on. Mathematically, this means calculating $\\text{sgn}(\\mathbf{x} \\cdot \\mathbf{w} + b)$. \n",
    "3. SVMs try to maximize the margin - the distance from the hyperplane to the closest point in the dataset\n",
    "4. We allow the datapoints to have *slack* and fall inside the margin, so that SVMs work with data that is not perfectly linearly seperable and can maximize the margin to most of the points. \n",
    "5. The optimal weights of an SVM are a linear combination of points that fall on and inside the margin - the *support vectors*. Making a prediction with an SVM means taking dot products with the *support vectors*.\n",
    "\n",
    "But you might be wondering - what if we have dataset that clearly isn't meant to be linearly separated? Consider the following dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_points = 100\n",
    "radii = np.random.uniform(1.5, 2.5, num_points)\n",
    "angles = np.random.uniform(0, 2 * np.pi, num_points)\n",
    "x = radii * np.cos(angles)\n",
    "y = radii * np.sin(angles)\n",
    "X1 = np.column_stack((x, y))\n",
    "\n",
    "radii = np.random.uniform(0, 1.0, num_points)\n",
    "angles = np.random.uniform(0, 2 * np.pi, num_points)\n",
    "x = radii * np.cos(angles)\n",
    "y = radii * np.sin(angles)\n",
    "X2 = np.column_stack((x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (3, 3))\n",
    "ax.scatter(X1[:, 0], X1[:, 1], label = \"Class 1\")\n",
    "ax.scatter(X2[:, 0], X2[:, 1], label = \"Class 2\")\n",
    "ax.set_xlabel(\"$x_1$\")\n",
    "ax.set_ylabel(\"$x_2$\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, this dataset isn't linearly separable - but it is separable by a circle. So what do we do? We can do trick is called *featurization*, where we come up with our own features! In this case, let's try creating 2 new features - $\\mathbf{x}_1^2$, and $\\mathbf{x}_2^2$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_featurized = X1 ** 2\n",
    "X2_featurized = X2 ** 2\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X1_featurized[:, 0], X1_featurized[:, 1], label = \"Class 1\")\n",
    "ax.scatter(X2_featurized[:, 0], X2_featurized[:, 1], label = \"Class 2\")\n",
    "ax.set_xlabel(\"$x_1^2$\")\n",
    "ax.set_ylabel(\"$x_2^2$\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is now linearly separable! So we see that **featurization** lets us transform data that can't be linearly separated into linearly separable data. Formally, we start out with features $\\mathbf{x} = [x_1, x_2]$, but then we now work in new feature space $\\phi(\\mathbf{x}) = [x_1^2, x_2^2]$\n",
    "\\\n",
    "\\\n",
    "Now, we could go around doing this for every dataset - we could try to come up with a new set of features that makes our dataset linearly separable. Now the problem is that it is really hard to manually featurize, and that if we come up with a whole bunch of new features, we'll need to compute the dot product in that high dimensional space - a highly time consuming process. If we started out with $\\mathbf{x}$ in 10 dimensions, and then $\\phi(\\mathbf{x})$ is in 100 dimensions (which is a realistic scenario), our dot products will take 10 times as long to compute!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lucky for us, there's a way out! Remember, our goal in an SVM is, for a feature mappings $\\mathbf{x} \\rightarrow \\phi(\\mathbf{x})$, to compute $\\phi(\\mathbf{x}) \\cdot \\phi(\\mathbf{z})$ - the dot product between $\\mathbf{x}$ and $\\mathbf{y}$ in a new, high dimensional feature space! As it turns out, we can often compute a **kernel**, such that:\n",
    "\n",
    "$$K(x, z) = \\phi(\\mathbf{x}) \\cdot \\phi(\\mathbf{z})$$\n",
    "\n",
    "And this kernel can be computed in the original, low dimensional feature space! For example, it turns out that the kernel between x and z in N dimensional space:\n",
    "\n",
    "$$K(x, z) = (x \\cdot z)^2$$\n",
    "\n",
    "Is equivalent to taking the dot product between an $N^2$ feature vectors $\\phi(\\mathbf{x})$ and $\\phi(\\mathbf{z})$!! So the **kernel trick** is this: for a non-linearly separable dataset, choose a kernel. Then, when we go to compute the dot products between a new point and the support vectors, we instead compute the **kernel** between them - this is equivalent to computing the dot product in high dimensional feature space! Then our prediction for a new point $\\mathbf{x}$ becomes:\n",
    "\n",
    "$$\\sum_{i=1}^{N} \\alpha_i y_i (\\mathbf{x}_i \\cdot \\mathbf{x}) + b \\rightarrow \\sum_{i=1}^{N} \\alpha_i y_i K(\\mathbf{x}_i, \\mathbf{x}) + b$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several popular kernels you'll see. The *Gaussian* kernel is:\n",
    "\n",
    "$$K(x, z) = exp(-\\gamma||x - z||^2)$$\n",
    "\n",
    "Where $\\gamma$ is another **hyperparameter** that we choose (not to be confused with the margin)! Let's take a look at how using a *Gaussian* kernel will let us create non-linear boundaries around our support vectors. I'll plot the support vectors below, as well as the separating \"hyperplane\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "from ipywidgets import interact, FloatSlider\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import clear_output\n",
    "\n",
    "x = np.linspace(-5, 5, 100)\n",
    "y = np.linspace(-5, 5, 100)\n",
    "support_vectors = np.array([[-2, 3], [0, 0], [3, 2]])\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "def plot_contour(w1, w2, w3, b, gamma):\n",
    "    weights = np.array([w1, w2, w3])\n",
    "    x_diffs = X - support_vectors[:, 0][:, np.newaxis, np.newaxis]\n",
    "    y_diffs = Y - support_vectors[:, 1][:, np.newaxis, np.newaxis]\n",
    "    kernels = np.exp(gamma * (-x_diffs ** 2 - y_diffs ** 2))\n",
    "    Z = np.sum(kernels * weights[:, np.newaxis, np.newaxis], \n",
    "               axis = 0) + b\n",
    "    ax.clear()\n",
    "    ax.scatter(support_vectors[:, 0], support_vectors[:, 1])\n",
    "    contour = ax.contour(X, Y, Z, \n",
    "                          levels=[0], colors='blue')\n",
    "\n",
    "\n",
    "w1_slider = FloatSlider(value=1.0, min=-1, max=1, step=0.1, description=\"a1\")\n",
    "w2_slider = FloatSlider(value=1.0, min=-1, max=1, step=0.1, description='a2')\n",
    "w3_slider = FloatSlider(value=0.0, min=-1, max=1, step=0.1, description='a3')\n",
    "gamma_slider = FloatSlider(value=1.0, min=0.01, max=2.0, step=0.1, description='gamma')\n",
    "\n",
    "\n",
    "interact(plot_contour, w1=w1_slider, w2=w2_slider, w3 = w3_slider, b = -0.5, gamma = gamma_slider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that by adjusting the weights and $\\gamma$ of our support vector, we are able to create a non-linear decision boundary! We also see that smaller values of $\\sigma$ means that the decision boundary is more sensitive to each individual data point, while larger values means that it is less sensitive. This will help us as we adjust our hyperparameters!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Fully Coded, Real World Use Case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have all the knowledge that we need to apply SVMs to a real world research example! In this example, we'll be working with a real world cancer dataset, which has been used in multiple research papers: The Wisconsin breast cancer datset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "cancer = datasets.load_breast_cancer()\n",
    "df_cancer = pd.DataFrame(np.c_[cancer['data'], cancer['target']], columns = np.append(cancer['feature_names'], ['target']))\n",
    "df_cancer.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset now consists of many more features than just the radius and perimeter. We see that there are 30 features, and we again try to predict the target - whether or not the tumor is malignant or benign. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step of data processing is to store the input features, and the target labels, as [numpy arrays](https://numpy.org/devdocs/user/quickstart.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_cancer.drop(['target'], axis = 1).to_numpy()\n",
    "y = df_cancer['target'].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate how the SVM is doing, we'll use the SVM to predict the labels of the points in the test set. Then, we'll see how closely those predictions match the actual labels of the testing set - that is, we'll find the **test error**. Scikit-learn, a free machine learning library in python, does a randomized training and testing split for us. The training set will be our labelled dataset that we let the SVM look at!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're giving the SVM the 80 percent of the data as training data, and then making SVM predict the labels of the reamining 20 percent, in order to test how well the SVM performs on data that we've never seen before. \n",
    "\\\n",
    "\\\n",
    "The next step is to perform what's called *feature scaling*. Feature scaling means that we'll scale all of the features of our dataset to occupy roughly the same range. This helps the SVM train. To see why feature scaling is important, imagine you had one feature that ranged from 0 to 1000, and another feature from 0 to 1, and the 2 features are equally important in determning the separating hyperplane. In this case, the SVM will need to find weights that differ roughly 1000 times in magnitude in order to capture the fact that the features are equally important. While the SVM will be able to do this, this will make training the SVM more time consuming. \n",
    "\\\n",
    "\\\n",
    "Luckily, Scikit-learn does feature scaling for us! We'll use the StandardScaler to shift each feature to have mean 0 and standard deviation 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice a slightly subtle point here: we fit *and* transform training data - that is, we decide how much to shift and scale each feature based on the training data (fitting), before actually applying that shifting and scaling (transforming). However, when we have test data, we only *transform* the test data. This is because we want to act like we've never seen the test data when we train our model, and thus don't use the information from the test set to determine the feature scaling. Remember, the goal is to test how well the SVM will generalize to new data that we've never seen before - and when we really have new data, we won't be able to access the data beforehand to help determing the feature scaling!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see how the SVM with a Gaussian kernel (called 'rbf' in sklearn), and C = 1 and and $\\gamma = 0.2$, is trained on the training set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf = SVC(kernel = 'rbf', C = 1, gamma = 0.2)\n",
    "clf.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's see how the SVM makes predictions on the training set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = clf.predict(X_test_scaled)\n",
    "test_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can look at the **accuracy** of the predictions - what percentage of the times the SVM predicts the correct classification on the test set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides accuracy, we can also look at 2 other metrics - the **precision** and the **recall**. The **precision** is defined as the total number of times that the ML algorithm correctly predicted a positive class - in our case, that a tumor is malignant - divided by the total number of times that an ML algorithm predicted a positive class. In other words, the precision tells us how often predictions of a tumor being malignant are correct. \n",
    "\n",
    "Meanwhile, the **recall** is defined by the total number of times that the ML algorithm correctly predicted a positive class divided by the total number of positive cases. In other words, the **recall** tells us how often malignant tumors are correctly identified by the SVM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PRECISION:\", metrics.precision_score(y_test, test_predictions))\n",
    "print(\"Recall:\", metrics.recall_score(y_test, test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's great that we can get pretty good predictions - but we just guessed values of C and $\\gamma$. How can we find the best values of C and $\\gamma$ - in other words, how can we tune our hyperparameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could see one possible way that we could do this - we could try out different values of C and $\\gamma$ until we find one that maximizes the test accuracy. So we deliver the best test accuracy, and we've optimized our SVM algorithm! Right?\n",
    "\n",
    "The problem with the above approach is that we've cheated - we're *using* the testing set to tune the value of k. Therefore, the test error no longer faithfully represents how well the algorithm performs on new, unseen data - we've already used the testing set to fit C and $\\gamma$. \n",
    "\n",
    "We can remedy this by further splitting the training set into training and **validation** sets. We'll first try to predict the labels of the *validation* set, and use this to tune C and $\\gamma$. Once we find a good value of C and $\\gamma$, we'll use the testing set to get an unbiased evaluation of the SVM performance. \n",
    "\n",
    "![Training, Validation, and Testing Image](https://b1739487.smushcdn.com/1739487/wp-content/uploads/2021/04/train-and-test-1-min-1.png?lossy=0&strip=1&webp=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, since our dataset is rather small, we don't have to just do the validation on a single split of training and validation. For a given value of C and $\\gamma$ We can do it several times, each time using a different part of the original training set as the validation set. This is called k-fold cross validation (image from Analytics Vidhya):\n",
    "\n",
    "<img src=\"https://editor.analyticsvidhya.com/uploads/57458kzw7d4mp9b45VFQN2wjKu1K8J9KrDh.png\" alt=\"drawing\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The good news is that Scikit-learn does cross validation for us! And what's more, since each time we get a new training and validation split, we also need to refit the StandardScaler, Scikit-learn lets us combine the scaler and the SVM into a **pipeline**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "clf = Pipeline([('Scaler', StandardScaler()),\n",
    "                ('Estimator', SVC(kernel = 'rbf', C = 1, gamma = 0.2))])\n",
    "scores = cross_val_score(clf, X_train, y_train, cv=5)\n",
    "print(f\"Cross validation scores: {scores}\")\n",
    "print(f\"Mean score: {np.mean(scores)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, all this cross validation only occurs for a single set of C and $\\gamma$. We want to try it out for a bunch of different values of C and $\\gamma$!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cross validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "best_C = -1\n",
    "best_gamma = -1\n",
    "best_accuracy = 0\n",
    "for C in [0.01, 0.1, 1, 10, 20, 30]:\n",
    "    for gamma in [0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1]:\n",
    "        clf = Pipeline([('Scaler', StandardScaler()),\n",
    "                        ('Estimator', SVC(kernel = \"rbf\", C = C, gamma = gamma, random_state = 42))])\n",
    "        scores = cross_val_score(clf, X_train, y_train, cv=5)\n",
    "        mean_score = np.mean(scores)\n",
    "        if mean_score > best_accuracy:\n",
    "            best_C = C\n",
    "            best_gamma = gamma\n",
    "            best_accuracy = mean_score\n",
    "        print(f\"C = {C}, gamma = {gamma}:\")\n",
    "        print(f\"Cross validation scores: {scores}\")\n",
    "        print(f\"Mean score: {mean_score} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best C: {best_C}, best gamma: {best_gamma}, best accuracy: {best_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've found the best hyperparameters through cross validation, we can go ahead and train the SVM with the best hyperparameters on the full training set, and use it to predict the labels of the test set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(kernel=\"rbf\", C = 10, gamma = 0.01)\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "test_predictions = clf.predict(X_test_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, test_predictions))\n",
    "print(\"PRECISION:\", metrics.precision_score(y_test, test_predictions))\n",
    "print(\"Recall:\", metrics.recall_score(y_test, test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a cautionary note - in practice, with all ML algorithms, we don't actually want to make predictions on the test set twice. We've done it here to illustrate the difference between hyperparameter tuning and not hyperparameter tuning - but in practice, you'll just tune hyperparameters, and then apply to the test set **once**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When to use an SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVMs are often faster to train than more complicated classification algorithms, such as neural networks. For this reason, SVMs are often useful on **small** or **intermediate** sized datasets. Once the datasets become larger, however, SVMs can become more expensive. This is because **training** an SVM requires that we compute the kernel between each one of our datapoints, which takes time that goes as the squared of the number of datapoints! Here are some general rules of thumb for training SVMs:\n",
    "\n",
    "- If the number of features is much greater than the number of training examples, don't use a kernel (our vanilla linear SVM)\n",
    "- If the number of features is much *less* than the number of training examples, and the number of training examples is an intermediate number (between roughly 10 and 10,000), try an SVM with a non-linear kernel \n",
    "- If the number of features is much less than the number of training examples, and the number of training examples is *large*, we can try creating or adding more features, and then using an SVM with no kernel. \n",
    "- If the number of features and number of training examples are large, or the number of training examples is really, really large, consider using another classification algorithm, such as a neural network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lecture, we went over the intuition, mathematics, and practicalities of using SVMs. We saw how SVMs arise naturally from drawing hyperplanes between classes of data, and from maximizing the distance from the hyperplanes to the data. We also saw how introducing slack and kernels helps us when the data is not perfectly linearly separable, or just not linearly separable at all. Finally, we saw how the SVM from Scikit-learn works, and how to tune hyperparameters for a real world problem. While there is still more to be said about how exactly SVMs are optimized (see Andrew Ng's [excellent lecture notes](https://see.stanford.edu/materials/aimlcs229/cs229-notes3.pdf) if you're curious), I hope that this tutorial provides you with enough knowledge to confidently use SVMs, and know what the SVM is really trying to accomplish! So go forth, and have fun classifying! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
