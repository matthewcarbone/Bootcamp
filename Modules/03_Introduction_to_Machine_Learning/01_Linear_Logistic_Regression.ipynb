{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7f592eb-ad45-414f-acad-d6e2a51a5f26",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Regression and Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3661abd0-a598-4f4e-95bc-ceb38c6815a8",
   "metadata": {},
   "source": [
    "_This notebook was originally written by [Matthew R. Carbone](https://www.bnl.gov/staff/mcarbone) for the WDTS RENEW Pathway Summer School: Fermilab and Brookhaven Summer School Exchange Program._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2eaf124-eb7b-41c7-b4f1-6792fd8fec3b",
   "metadata": {},
   "source": [
    "In this tutorial, we're going to go over the fundamentals of regression and classification, which are the two most common types of supervised learning. We will also discuss some of the best practices in machine learning, such as a train-validation-testing split. In regression problems, the objective is to learn a _continuous_ output. In classification problems, the objective is to learn a _discrete_ output. Here are some examples:\n",
    "- Predicting the cost of a house from its properties, such as square footage, number of bathrooms, etc. is a _regression_ problem.\n",
    "- Whether or not an image is of a cat or dog is a _classification_ problem.\n",
    "- Predicting the type of animal in an image is a _classification_ problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df42443-4fbb-47a6-bb3a-b96c92f5e675",
   "metadata": {},
   "source": [
    "**Learning objectives:**\n",
    "- Understand a variety of regression and classification algorithms.\n",
    "- Start to explore some of the fundamental concepts in machine learning, such as splitting data, overfitting, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e132cd22-31cc-4bb0-8823-a6d6c2e4704a",
   "metadata": {},
   "source": [
    "Regression and classification problems can be solved via a variety of different methods (or _models_). In this tutorial, we're going to go over the following types of models, which will form the backbone of your understanding for e.g. neural networks, and other types of machine learning, later on.\n",
    "- Linear regression\n",
    "- Polynomial regression\n",
    "- Logistic regression\n",
    "\n",
    "There are numerous [other types of models](https://www.listendata.com/2018/03/regression-analysis.html#Linear-Regression) which we simply won't have the time to dive into, but paradigmatically, the objective of all of these models is the same. Given some input, predict some output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40ab799-d376-4921-b58d-f9f9330ed9cc",
   "metadata": {},
   "source": [
    "## Ingredients for regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2bfca4-511b-4233-b141-f390c5079fc4",
   "metadata": {},
   "source": [
    "There are a few \"ingredients\" to always consider when approaching a regression problem.\n",
    "- Your available data (\"dataset\")\n",
    "- Your choice of model (\"model\")\n",
    "- How you choose to fit the model to the data (\"optimizer\")\n",
    "- An indicator for how well your model fits the data (\"loss function\"/\"metric\"/\"criterion\")\n",
    "\n",
    "We will discuss all of these components today."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace206c5-00d1-4175-9ac3-23b7ee8a18e4",
   "metadata": {},
   "source": [
    "## Other resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edaf978-57f3-46fb-9824-432381d64793",
   "metadata": {},
   "source": [
    "- [Andrew Ng's flagship Coursera course on machine learning](https://www.coursera.org/specializations/machine-learning-introduction)\n",
    "- [Intro to regression analysis](https://towardsdatascience.com/introduction-to-regression-analysis-9151d8ac14b3)\n",
    "- [15 types of regression](https://www.listendata.com/2018/03/regression-analysis.html#Linear-Regression)\n",
    "- [Gradient descent tutorial](https://machinelearningmind.com/2019/10/06/gradient-descent-introduction-and-implementation-in-python/)\n",
    "- [Linear regression gradient descent tutorial](https://towardsdatascience.com/linear-regression-using-gradient-descent-97a6c8700931)\n",
    "- [Logistic regression tutorial](https://towardsdatascience.com/logistic-regression-detailed-overview-46c4da4303bc)\n",
    "- [Another logistic regression tutorial](https://www.kaggle.com/code/prashant111/logistic-regression-classifier-tutorial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e143024-c218-4aa8-8d7c-3756ea96dfce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initial imports, and some default plotting settings.\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "mpl.rcParams['mathtext.fontset'] = 'stix'\n",
    "mpl.rcParams['font.family'] = 'STIXGeneral'\n",
    "plt.rc('xtick', labelsize=12)\n",
    "plt.rc('ytick', labelsize=12)\n",
    "plt.rc('axes', labelsize=12)\n",
    "mpl.rcParams['figure.dpi'] = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e177bded-5e38-4c6b-a71e-52de42f930ee",
   "metadata": {},
   "source": [
    "# Linear regression and gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ec192d-61b3-4e78-aa70-e11392a8eebe",
   "metadata": {},
   "source": [
    "Let's begin with the simplest form of regression: that of fitting a line to data. We can recast this problem as learning a function $f(x) = y,$ where the form of $f$ is simply the familiar $f(x) = mx + b.$ Given a dataset $\\{x_i, y_i\\}$, we can \"learn\" the coefficients $m$ and $b$ that best model the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ace82bd-f62b-4c7b-b424-502780c3795c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def linear_model(x, m, b):\n",
    "    return m * x + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074d047f-0326-447c-a77d-876720ff1697",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def linear_data_with_noise(seed=123, scale=0.5, N=100, slope=2.4, y_intercept=0.8):\n",
    "    np.random.seed(seed)\n",
    "    x = np.linspace(-1, 1, N)\n",
    "    y = linear_model(x, slope, y_intercept) + np.random.normal(scale=scale, size=(N,))\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0075f0-3b09-4d0e-9969-2b3669baafda",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = linear_data_with_noise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfacd60-7e84-4ee5-9fca-486dc5a05990",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(3, 2))\n",
    "\n",
    "ax.scatter(x, y)\n",
    "ax.set_xlabel(\"$x$\")\n",
    "ax.set_ylabel(\"$y$\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c30ce42-0f9e-4595-93c3-c8e7a905a590",
   "metadata": {},
   "source": [
    "Suppose we fix $m$ and $b$ to some values $m_0$ and $b_0$. We now have a model which is completely defined, and given some value for $x$, we can predict $y.$ But how do we know if these are good choices? We need to define a metric, or a measure of how well the model fits the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a012648e-cbd8-4d5f-93ee-58d4b52052a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def criterion(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7f98e2-bea8-44b6-a764-06e93989d155",
   "metadata": {},
   "source": [
    "Above, our `criterion` is called the average mean squared error (MSE). The square root of this is the \"root-mean-squared\" error, which you are likely familiar with. There are many ways to choose which criterion you want to use, but for now the MSE will suffice. In machine learning lingo, it's called a \"hyper parameter\" (which we can discuss more later). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9630f25d-04a4-4354-80f4-dee6e36fae50",
   "metadata": {},
   "source": [
    "## Evaluating an arbitrary model using a \"random\" optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c5ba7c-598d-4d5d-872f-4bdbd6dd0588",
   "metadata": {},
   "source": [
    "We know what the ground truth slope `m` and y-intercept `b` actually are, but let's pretend we don't, and evaluate a set of linear models against the data that we have. We can take a bunch of values for `m0` (guesses for the slope) and `b0` (guesses for the y-intercept), create a linear model with those parameters, and evaluate those against the actual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41041931-7a83-48a8-89e1-8fd00da8de66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "m0 = np.random.random(size=100) * 6 - 3  # 100 random numbers between -3 and 3\n",
    "b0 = np.random.random(size=100)          # 100 random numbers between 0 and 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb35b064-0388-473e-b7fc-7eef131bb705",
   "metadata": {},
   "source": [
    "Let's try every one of these combinations for `m0` and `b0`, and valuate the model against our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de26ed2-08b6-4303-b512-1be843f1837c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5ec25a-2c68-4164-a54e-bedeb0626e11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "m0b0 = list(product(m0, b0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3011f10-4eb2-4ee5-914d-6ddb994874a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    \"m0\": [params[0] for params in m0b0],\n",
    "    \"b0\": [params[1] for params in m0b0],\n",
    "    \"criterion\": [criterion(y, linear_model(x, *params)) for params in m0b0]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d401523c-ece0-448e-b5ff-a65eda1ab69f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8746c23e-7fd0-4477-b073-dadff00f85ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "argmin = df[\"criterion\"].argmin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da02cd92-a4c4-42a3-baec-85dc446b985c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.iloc[argmin, :]  # What happened here?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cf7150-c710-481b-962e-f9618c0d3109",
   "metadata": {
    "tags": []
   },
   "source": [
    "## A smarter optimizer: gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abeab12c-e4a6-460c-aa8a-baa12aa5c76d",
   "metadata": {},
   "source": [
    "It stands to reason that just randomly guessing parameters is probably not the most effective way of finding the right guess. Can we do better? Absolutely! [Gradient descent](https://en.wikipedia.org/wiki/Gradient_descent), and its family of related methods (you may have heard of [Adam](https://arxiv.org/abs/1412.6980)), can be used to find an optimal set of parameters for arbitrary models.\n",
    "\n",
    "Gradient descent, and its related methods, are all about finding the optima of functions. For example, if I give you a function $f(x) = (x-2)^2$ and ask you to find its minimum, how would you do this. From calculus, you might know how. You also might recognize this is a simple parabola and know the answer right away. But what about some arbitrary function, possibly with many _local_ minima, and possibly a _huge_ number of parameters (in machine learning, this number can easily be greater than a billion)? Gradient descent is a method for finding minima numerically.\n",
    "\n",
    "Given a function $f(x)$ and its derivative $\\nabla f(x),$ the general rule of gradient descent is that to find a point closer to the actual minimum of $f,$ we follow its gradient:\n",
    "\n",
    "$$x_{n+1} = x_n - \\gamma \\nabla f(x_n).$$\n",
    "\n",
    "This is perhaps easiest to see through an example. Let's consider the function $f(x) = (x-2)^2,$ which of course has derivative $df(x)/dx = 2(x-2).$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca78118-a5ef-4cfc-8ace-9096c71d287c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gradient_descent(x0, f_prime, gamma, N=100):\n",
    "    \"\"\"The gradient descent algorithm for a scalar function.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x0 : float\n",
    "        The initial guess for the minimum.\n",
    "    f_prime : callable\n",
    "        The derivative of the function of interest.\n",
    "    gamma : float\n",
    "        Learning rate.\n",
    "    N : int\n",
    "        Number of iterations.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        A list of the values of x found during gradient descent.\n",
    "    \"\"\"\n",
    "    \n",
    "    guesses = [x0]\n",
    "    for _ in range(N):\n",
    "        x_n = guesses[-1]\n",
    "        guesses.append(x_n - gamma * f_prime(x_n))\n",
    "    return guesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f87224-49ce-4561-b700-0dac55129be5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "points = np.array(gradient_descent(\n",
    "    x0=10.0,\n",
    "    f_prime=lambda x: 2.0 * (x - 2.0),\n",
    "    gamma=0.1,\n",
    "    N=100\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b2d217-0f10-4e61-b7c3-54f1689a9138",
   "metadata": {},
   "source": [
    "We of course know that the true minimum of this function is at $x=2,$ but lets plot how well the `gradient_descent` function finds this minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7a98ee-8b4f-483e-88da-d0030747c179",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cmap = mpl.colormaps[\"viridis\"].resampled(len(points))\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(3, 1))\n",
    "\n",
    "xgrid = np.linspace(-1, 15, 100)\n",
    "ax.plot(xgrid, (xgrid - 2.0)**2, \"k-\", label=r\"$f(x)=(x-2)^2$\")\n",
    "ax.scatter(points, (points - 2.0)**2, c=[cmap(ii) for ii in range(len(points))], s=10)\n",
    "ax.axvline(2.0, zorder=-1, color=\"red\", label=\"min $f(x)$\")\n",
    "ax.legend(frameon=False, fontsize=5)\n",
    "\n",
    "ax.set_xlabel(\"$x$\")\n",
    "ax.set_ylabel(\"$f(x)$\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a9f161-69dc-4913-a631-fcdedba19dfa",
   "metadata": {},
   "source": [
    "Play around with the above code a bit. Here are some examples of things you can try.\n",
    "- What happens if you make $\\gamma$ really large?\n",
    "- What happens if you make $\\gamma$ really small?\n",
    "- What happens if you change $x_0$?\n",
    "- What happens if you change the function? For example, try $f(x) = \\sin x$ (remember the derivative from calculus?). How sensitive is this procedure to your choice of $x_0$?\n",
    "\n",
    "Here's a [video](https://upload.wikimedia.org/wikipedia/commons/transcoded/4/4c/Gradient_Descent_in_2D.webm/Gradient_Descent_in_2D.webm.720p.vp9.webm) of what gradient descent is doing with a lot of points. This arbitrary function is two dimensional. If you are not sure what's happening here, ask!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f784c3-bd5a-4b50-ad43-79e138b37f32",
   "metadata": {},
   "source": [
    "## Minimizing the loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fe7ca3-01b0-4006-a7c0-c1e82aad173a",
   "metadata": {},
   "source": [
    "Now we put two pieces together. As a reminder, the criterion, which we will now refer to as a loss function, is a measure of how different two data points are. We wish to minimize the average of this loss function when doing machine learning. \n",
    "\n",
    "For example, consider the linear fit from before. Our ground truth \"training data\" is $\\{x_i, y_i\\},$ and our model is simply $f(x; m, b) = mx + b$. We want to _minimize_ the difference between the ground truth values and predicted values on our dataset:\n",
    "\n",
    "$$ L(m, b) = \\frac{1}{N} \\sum_{i=1}^N (y_i - f(x_i; m, b))^2 $$\n",
    "\n",
    "Specifically, we want to minimize $L$ with respect to $m$ and $b.$ Stated slightly differently, we want to find the corresponding values of $m, b$ such that $L$ is minimized. We just learned of a way to do this, gradient descent! However, to use gradient descent, we need the partial derivatives of $L$ with respect to all parameters of interest. Luckily, $L$ is differentiable analytically.\n",
    "\n",
    "NOTE: ALL machine learning models trained with the family of gradient descent methods have to have analytically known derivatives. This includes neural networks!\n",
    "\n",
    "The derivatives are:\n",
    "\n",
    "$$ \\frac{\\partial L(m, b)}{\\partial m} = -\\frac{2}{N}\\sum_{i=1}^N (y_i - f(x_i; m, b)) x_i$$\n",
    "\n",
    "and\n",
    "\n",
    "$$ \\frac{\\partial L(m, b)}{\\partial b} = -\\frac{2}{N}\\sum_{i=1}^N (y_i - f(x_i; m, b)).$$\n",
    "\n",
    "Following the rule from before, the update rules for $m$ and $b$ are:\n",
    "\n",
    "$$ m_{n+1} = m_n - \\gamma \\frac{\\partial L(m, b)}{\\partial m}$$\n",
    "\n",
    "and\n",
    "\n",
    "$$ b_{n+1} = b_n - \\gamma \\frac{\\partial L(m, b)}{\\partial b}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728f37f3-a06f-4922-baae-a0d111c711a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_linear_model(x, y, m0, b0, gamma=0.1, N=100):\n",
    "    b = [b0]\n",
    "    m = [m0]\n",
    "\n",
    "    for _ in range(N):\n",
    "        \n",
    "        y_pred = linear_model(x, m[-1], b[-1])\n",
    "        \n",
    "        # Calculate the derivatives of the loss function\n",
    "        dLdm = -2.0 / len(x) * np.sum(x * (y - y_pred))\n",
    "        dLdb = -2.0 / len(x) * np.sum(y - y_pred)\n",
    "        \n",
    "        # Update\n",
    "        m.append(m[-1] - gamma * dLdm)\n",
    "        b.append(b[-1] - gamma * dLdb)\n",
    "\n",
    "    return m, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13311f6-184f-4038-b818-993ab6ddcdb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "m, b = gradient_descent_linear_model(x, y, m0=-1.9, b0=-5.0, gamma=0.1, N=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627dbc94-d9bf-445d-8f3e-85c70cdb53ce",
   "metadata": {},
   "source": [
    "If we plot the convergence of `m` and `b` as a function of \"epoch\" (or number of full passes through the training data) we can see a very fast convergence of the parameters to the true parameters, which are $m = 0.8$ and $b=2.4$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957690ef-36e6-4988-af78-9db493e9e9fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(3, 1))\n",
    "\n",
    "ax.axhline(0.8, color=\"black\", linewidth=0.5)\n",
    "ax.axhline(2.4, color=\"black\", linewidth=0.5)\n",
    "\n",
    "ax.plot(m, \"ro-\", linewidth=0, markersize=0.5, label=\"$m$\")\n",
    "ax.plot(b, \"bo-\", linewidth=0, markersize=0.5, label=\"$b$\")\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d4b3e5-56ae-4a2b-a67d-285f5ede2881",
   "metadata": {},
   "source": [
    "We can also plot the linear fit itself as a function of epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b217987a-c9c8-478a-8a78-752396f31818",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cmap = mpl.colormaps[\"viridis\"].resampled(len(m))\n",
    "grid = np.linspace(-1, 1, 100)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(3, 2))\n",
    "\n",
    "\n",
    "\n",
    "for ii, (mm, bb) in enumerate(zip(m, b)):\n",
    "    ax.plot(grid, linear_model(grid, mm, bb), color=cmap(ii))\n",
    "    \n",
    "ax.scatter(x, y, zorder=1)\n",
    "\n",
    "ax.set_xlabel(\"$x$\")\n",
    "ax.set_ylabel(\"$y$\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79799570-5536-4541-b182-6a8ce30b9391",
   "metadata": {},
   "source": [
    "As with the previous section, understanding what's going on here is critical. Once again, ask and answer (discuss with your neighbors), what is going on here? **If you can understand this simple example, you will be able to understand all of gradient-based machine learning that we present as part of this tutorial series!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cb0c66-cbfc-4617-aa35-916c21867148",
   "metadata": {},
   "source": [
    "## California housing dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d1a992-0846-46c4-839c-b6abefecbb7d",
   "metadata": {},
   "source": [
    "Let's apply the knowledge we've gained from the previous sections on a \"real\" problem. We'll be using the [California Housing Dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_california_housing.html), which is a classic teaching example in machine learning. We'll also loosely follow along with this [tutorial](https://towardsdatascience.com/introduction-to-regression-analysis-9151d8ac14b3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbcce43-8ac7-467f-b16b-211f37962b2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a017a7-eab1-4019-b62d-2d99c66c64f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = fetch_california_housing()\n",
    "\n",
    "# Put the data into a cleaner format:\n",
    "df = pd.DataFrame({key: data[\"data\"].T[ii] for ii, key in enumerate(data[\"feature_names\"])})\n",
    "df[data[\"target_names\"][0]] = data[\"target\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bbcaea-f8be-4120-9ba3-221b0000073a",
   "metadata": {},
   "source": [
    "Before we fit anything, let's do some exploratory data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab39e93-09b5-4c6c-a42d-aa83caf2f493",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[\"MedHouseVal\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43577c9-2aa4-4daa-8b30-f444675edeed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[\"MedHouseVal\"].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4ec9b7-b4b7-44de-be7a-f9c68b5e73aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c3059c-fa7d-4fd9-a048-5862b59acc17",
   "metadata": {},
   "source": [
    "The strongest correlation between our inputs and outputs comes from the `HouseAge`, `AveRooms` and especially `MedInc`. We'll use these three variables as part of a multilinear regression model:\n",
    "\n",
    "$$ f(x_1, x_2, x_3) = c_0 + c_1 x_1 + c_2 x_2 + c_3 x_3, $$\n",
    "\n",
    "where $x_1, x_2, x_3$ are the values of the three input variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32264948-2225-47a4-a844-13d948e8ab53",
   "metadata": {},
   "source": [
    "Although there is not much linear correlation between latitude and longitude and our target, we can still make a pretty plot. Kinda looks like California!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f875d01a-fb1c-413f-9aed-a8dbabd3e99b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(2, 2))\n",
    "\n",
    "\n",
    "ax.scatter(df['Latitude'], df['Longitude'], c=df['MedHouseVal'], s=df['Population']/1000)\n",
    "cbar = fig.colorbar(mpl.cm.ScalarMappable(), ax=ax)\n",
    "ax.set_xlabel(\"Latitude\")\n",
    "ax.set_ylabel(\"Longitude\")\n",
    "cbar.set_label(\"MedHouseVal\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e2deb3-3063-4ff7-bdbb-86e11f457954",
   "metadata": {},
   "source": [
    "## Training and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862d7715-8146-45ef-a238-c820253011e3",
   "metadata": {},
   "source": [
    "In machine learning, and in any data-driven modeling, we have to \"fit\" the model on some dataset, but then we want an unbiased estimation of how the model will perform on data it has not yet seen before. We can easily do this with the `sci-kit learn` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21020ced-6af9-4e3f-9725-ab2e42feb128",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = df[[\"HouseAge\", \"AveRooms\", \"MedInc\"]].to_numpy()\n",
    "y = df[\"MedHouseVal\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457563bf-99ff-4ba0-a8ba-ab33c5c596f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8695b3ea-4400-4cee-88c8-3c669b3d8f59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e616beab-2fe8-4027-9cde-40f443b30585",
   "metadata": {},
   "source": [
    "Now let's train a linear model. Since you've already done this the hard way, now we can do it the easy way (using existing libraries)! It's also relevant to note that unlike in most machine learning problems (which use non-linear models), linear models can be fit analytically (and quickly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde431fa-5d6b-4fc8-941c-269e98b7c4fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d87e768-3fd8-4124-b710-6e6448fffbb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae9c344-4c53-4926-8441-83328aaeed3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae18d457-922b-4f4e-aad3-623c8184c577",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13b6c06-4388-4324-a0e2-208969c13b82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1c310e-a6a1-41cc-a7eb-e610db3f8677",
   "metadata": {},
   "source": [
    "From the trained model parameters, the linear equation is approximately\n",
    "\n",
    "$$ f(x_1, x_2, x_3) = 0.016 + 0.017 x_1 - 0.025 x_2 + 0.443 x_3 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682862af-3768-43d5-820c-cc40c541ac67",
   "metadata": {},
   "source": [
    "As before, we want to quantify the degree of fit. We can once again use the criterion to determine the quality of the fit. Let's do this on both the training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad8b202-2822-42ca-a635-80ecd074bffc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "criterion(y_train, model.predict(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54889035-e309-4662-9b59-85c395ddd3a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "criterion(y_test, model.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54a5617-0d39-4f38-8d2f-a2ad78131d7f",
   "metadata": {},
   "source": [
    "Given that the range of the target is roughly 5, this is not a terrible model. We can likely do better by including more variables, or by making our model more complex (which we will explore in future tutorials)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5799900-0acd-482e-9b50-0adcff693828",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfa8635-07c5-4268-bcea-3d1ecd6d5064",
   "metadata": {},
   "source": [
    "Logistic regression is a slight modification of linear regression that allows for categorical targets. There are multiple types of logistic regression, but for this tutorial, we will focus on _binary_ logistic regression: when the target value can be \"on or off\": 0 or 1.\n",
    "\n",
    "Consider the previous linear regression model of a single variable: $f(x) = mx + b$. The range, or set of possible values, that $f(x)$ can take on is infinite: $f(x) \\in (-\\infty, \\infty).$ This is appropriate, in general, for regression problems in which the output can take on any value, but in classification problems, we want the model to essentially represent the probability of being one of the two classes.\n",
    "\n",
    "So, for logistic regresion, we add an extra step:\n",
    "\n",
    "$$g(x) = \\sigma(f(x))$$\n",
    "\n",
    "where \n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "is called the _sigmoid function_. The sigmoid function has the property of being bounded between 0 and 1, making it a convenient choice for our purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b97fa7-324b-49d3-be75-ce65eebd160c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24793c5-bf7c-4607-ab64-9fba47660048",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(3, 1))\n",
    "\n",
    "grid = np.linspace(-6, 6, 100)\n",
    "ax.plot(grid, sigmoid(grid))\n",
    "ax.set_xlabel(\"$x$\")\n",
    "ax.set_ylabel(\"$\\sigma(x)$\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f78c6f-d35f-4ee6-be31-b7fd4cafcd40",
   "metadata": {},
   "source": [
    "The output of $\\sigma(x)$ is of course continuous. How can we get a discrete output from this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bf09f9-29a6-4eee-b09f-63ad010f7c1f",
   "metadata": {},
   "source": [
    "## New dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7328c3-0694-486f-8ed9-0814a7c4c84e",
   "metadata": {},
   "source": [
    "We'll use the [Iris Dataset](https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html), downloaded from sklearn, for this section. From sklearn:\n",
    "\n",
    "> This data sets consists of 3 different types of irises’ (Setosa, Versicolour, and Virginica) petal and sepal length, stored in a 150x4 numpy.ndarray. The rows being the samples and the columns being: Sepal Length, Sepal Width, Petal Length and Petal Width.\n",
    "\n",
    "The targets of this dataset are the type of flower, which we will represent as _categorical variables_ 0, 1, and 2. Since we're doing _binary_ logistic regression, we're going to do what is called a \"one vs. all\" approach, and see if we can predict whether or not an input corresponds to class 0, or not class 0 (either 1 or 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74725cda-04cb-47a3-b337-3c014b9bf6a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cce2b5e-e517-46d2-b387-00fffe77ee9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386e8fba-cfcf-421c-a6b8-ce0bbb36a982",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = iris[\"data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb32303-3d89-405b-a8a5-5b1f79622c1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y = iris[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98898f3f-4425-42dc-be64-7e4ed3922d42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# one vs all!\n",
    "y[y == 2] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9891952-9fb9-4f94-b8c3-587d41a50201",
   "metadata": {},
   "source": [
    "## New loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7bb474-0eaf-4717-b242-2fed788b15bc",
   "metadata": {},
   "source": [
    "While you certainly can use the mean squared error criterion from before, there is actually a better choice: it's called the _cross-entropy loss function_. It turns out that for logistic regression, the cross entropy loss is _convex_, meaning we are guaranteed to converge to the global minimum and find the absolute best fit of our model. The cross entropy loss is given by:\n",
    "\n",
    "$$J(\\hat{y}, y) = -y \\ln \\hat{y} - (1-y) \\ln (1-\\hat{y}),$$\n",
    "\n",
    "and the total loss over the dataset:\n",
    "\n",
    "$$L(m, b) = \\frac{1}{N} \\sum_{i=1}^N J(g(x_i), y_i).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1737079-cd7c-4871-826d-fb39fe6da13d",
   "metadata": {},
   "source": [
    "It's worthwhile to check your understanding at this point and try to answer the following questions:\n",
    "\n",
    "- Describe the properties of this new loss function.\n",
    "- What happens when $y=0$ or $\\hat{y}=0$?\n",
    "- What happens when $y=1$ or $\\hat{y}=1$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e219ae5-a35d-480a-a454-55304644e98d",
   "metadata": {},
   "source": [
    "## Gradient update rule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ccf18c-8203-4f52-b348-30117c633934",
   "metadata": {},
   "source": [
    "As before, we can take the derivative of $L$ with respect to its parameters (which are implicitly in $g$) to figure out our update rule for gradient descent.\n",
    "\n",
    "Remember, our functions are\n",
    "\n",
    "$$f(x) = c_0 + c_1x $$\n",
    "\n",
    "(which is the same as $f(x) = mx + b$ but written in a slightly more general form, you'll see why later), and\n",
    "\n",
    "$$g(x) = \\sigma(f(x)).$$\n",
    "\n",
    "Let's work through it. Using chain rule,\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial c_i} = \\frac{1}{N} \\sum_{i=1}^N \\frac{\\partial J}{\\partial \\sigma} \\frac{\\partial \\sigma}{\\partial f} \\frac{\\partial f}{\\partial c_i}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744b5bc9-0262-4b15-9581-7fd981968590",
   "metadata": {},
   "source": [
    "Working through each of these partial derivatives is a bit tedious, but it's not too bad:\n",
    "\n",
    "$$J(\\sigma, y) = -y \\ln \\sigma - (1-y)\\ln (1-\\sigma) \\Rightarrow  \\frac{\\partial J}{\\partial \\sigma} = -\\frac{y}{\\sigma} + \\frac{1-y}{1-\\sigma}$$\n",
    "\n",
    "$$\\sigma(f) = \\frac{1}{1 + e^{-f}} \\Rightarrow \\frac{\\partial \\sigma}{\\partial f} = \\frac{e^f}{(1+e^f)^2} = \\sigma(f) (1-\\sigma(f))$$\n",
    "\n",
    "$$\\frac{\\partial f}{\\partial c_i} = x_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bb9e65-a935-4f59-8dbb-b70dea11e713",
   "metadata": {},
   "source": [
    "Code up the model training algorithm for a logistic regressor and use it on the iris dataset. If you don't have enough time, use the built in one in Scikit Learn!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
